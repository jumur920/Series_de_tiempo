[["index.html", "Pronóstico de Ventas de Café en Máquinas Expendedoras Capítulo 1 Introducción 1.1 Justificación de la Elección 1.2 Descripción de la Información a Utilizar 1.3 Fuentes y Permisos de Uso", " Pronóstico de Ventas de Café en Máquinas Expendedoras Luisa Angélica Isaza Sanabria - Juan Andrés Murillo Cadena - Carlos Fabián Villa Infante 2025-06-09 Capítulo 1 Introducción Durante este curso de series de tiempo, hemos decidido trabajar las ventas de café en una máquina expendedora. Detrás de cada café que alguien compra, hay patrones de consumo, hábitos y decisiones que se repiten en el tiempo. Analizar esta información nos permite aplicar modelos de pronóstico reales, útiles y con impacto directo en la toma de decisiones comerciales y operativas. Poder anticipar cuánto café se va a vender en los próximos días, semanas o meses es clave para mejorar la experiencia del cliente, reducir pérdidas y aumentar la eficiencia. 1.1 Justificación de la Elección El café es una de las bebidas más consumidas en todo el mundo, y las máquinas expendedoras son una forma práctica de acceder a él. Nos pareció un caso ideal porque: 1. Ayuda a planificar mejor los inventarios: Prever la demanda permite tener siempre lo justo: ni mucho producto que termine vencido, ni tan poco que dejemos de vender. 2. Hace más eficientes las operaciones: Si sabemos cuándo se vende más café, podemos organizar mejor las recargas y los mantenimientos, ahorrando tiempo y dinero. 3. Permite personalizar promociones:Detectar días u horarios de baja demanda ayuda a lanzar promociones en momentos estratégicos. 4. Mejora la experiencia de quienes compran: Asegurar que los productos favoritos estén disponibles en los momentos clave mejora la satisfacción y fideliza al cliente. 1.2 Descripción de la Información a Utilizar Vamos a utilizar un dataset llamado “Coffee Sales”, publicado por Yaroslav Isaienkov en la plataforma Kaggle. Esta base de datos contiene registros reales de ventas desde marzo de 2024 y sigue actualizándose semanalmente. El dataset incluye: • La fecha y hora de cada transacción • El tipo de café vendido • La cantidad y el método de pago • Información detallada sobre el producto y las preferencias del cliente Todo esto en archivos están en formato .csv que son muy fáciles de trabajar y analizar, ideales para aplicar modelos de series de tiempo. 1.3 Fuentes y Permisos de Uso Una gran ventaja de este dataset es que está disponible bajo una licencia de dominio público (CC0),(Isaienkov (2025)) lo que significa que se puede usar libremente con fines educativos, de análisis y sin restricciones. Además, todos los datos han sido recolectados de forma anónima a partir de informes de la propia máquina expendedora. "],["análisis-grafico-de-series-de-tiempo.html", "Capítulo 2 Análisis grafico de series de tiempo 2.1 Metodología 2.2 Estacionariedad, Diferenciación y Transformaciones 2.3 Conclusiones", " Capítulo 2 Análisis grafico de series de tiempo Este es un análisis temporal de las ventas diarias de una máquina de café, con el objetivo de identificar patrones, tendencias y ciclos estacionales que permitan optimizar la gestión del negocio. Los datos incluyen información sobre la fecha y hora de las ventas, el medio de pago (efectivo o tarjeta), el valor de cada transacción y el tipo de café vendido. El análisis se centra en la variable valor_total (suma de ventas por día) y utiliza tres herramientas principales: el promedio móvil, la función de autocorrelación (ACF) y la descomposición STL. 2.1 Metodología 2.1.1 Datos Los datos abarcan un periodo de 385 días, desde abril 01 de 2024 hasta abril 23 de 2025. La variable analizada, valor_total, representa la suma de las ventas diarias (en dolares). 2.1.2 Promedio móvil Promedio móvil: Se calculó un promedio móvil de 7 días para suavizar las fluctuaciones diarias y destacar tendencias generales en las ventas. La siguiente gráfica muestra las ventas diarias (línea azul) junto con el promedio móvil (línea naranja). Figure 2.1: Ventas diarias con promedio móvil La gráfica muestra una alta variabilidad en las ventas diarias, con picos que alcanzan hasta 800 y caídas cercanas a 0. El promedio móvil revela las siguientes tendencias: Abril-julio 2024: Las ventas promedio crecen de ~300 a ~400, indicando un aumento en la demanda. Julio-octubre 2024: Alcanzan un pico de ~500, mostrando un periodo de alta demanda, posiblemente por festividades o factores estacionales. Octubre 2024-enero 2025: Disminuyen a ~300, reflejando una caída en las ventas durante el invierno. Enero-abril 2025: Se recuperan y estabilizan en ~400, indicando una mejora en la primavera. Esto sugiere una tendencia estacional a largo plazo, con un pico en octubre y una caída en invierno. 2.1.3 Función de Autocorrelación (ACF) La función de autocorrelación (ACF) mide la correlación de las ventas diarias consigo mismas en diferentes rezagos, ayudando a identificar patrones temporales y ciclos estacionales. Figure 2.2: Autocorrelación de ventas La gráfica ACF muestra: Rezagos 1 a 6: Autocorrelaciones significativas (~0.3 a 0.4), indicando una dependencia a corto plazo. Las ventas de un día están correlacionadas con las de los días anteriores, con un efecto que disminuye gradualmente. Rezago 7: Un pico significativo (~0.4), confirmando un ciclo semanal. Esto indica que las ventas tienen un patrón que se repite cada 7 días (por ejemplo, mayor demanda los fines de semana). Rezagos 8 a 14: Autocorrelaciones más pequeñas pero aún significativas, con otro pico en el rezago 14 (segundo ciclo semanal), reforzando el patrón estacional. Este ciclo semanal sugiere que las ventas varían según el día de la semana. 2.1.4 Descomposición STL La descomposición STL separa la serie temporal en tres componentes: tendencia, estacionalidad (ciclo semanal) y residuo. La descomposición STL revela los siguientes patrones: Tendencia: Similar al promedio móvil, muestra un aumento inicial (abril-julio 2024), un pico en octubre (500), una caída en invierno (300), y una recuperación en primavera (~400). Esto confirma una tendencia estacional a largo plazo. Estacionalidad: Oscila entre -20 y 20, con un ciclo que se repite cada 7 días, confirmando el patrón semanal identificado por la ACF. Aunque el efecto estacional es pequeño, indica variaciones según el día de la semana (por ejemplo, mayor demanda los fines de semana). Residuo: Varía entre -300 y 300, mostrando una alta variabilidad. Esto indica que hay fluctuaciones significativas en las ventas que no se explican por la tendencia ni la estacionalidad, posiblemente debido a eventos aleatorios (festivos, promociones, cierres). 2.2 Estacionariedad, Diferenciación y Transformaciones Se analizó la estacionariedad de las ventas y se evaluó la necesidad de diferenciación y transformaciones para controlar tendencia y variabilidad. 2.2.1 Análisis de Estacionariedad La estacionariedad se evaluó con la prueba ADF (Augmented Dickey-Fuller). Una serie se considera estacionaria si su media y varianza son constantes en el tiempo. ## ## Augmented Dickey-Fuller Test ## ## data: ts_ventas ## Dickey-Fuller = -3.5858, Lag order = 7, p-value = 0.0345 ## alternative hypothesis: stationary El valor p de la prueba ADF para la serie original es 0.0345. Dado que el valor p es menor que 0.05, rechazamos la hipótesis nula de no estacionariedad, indicando que la serie es estacionaria. Esto significa que la serie no requiere diferenciación, aunque la descomposición STL y el promedio móvil muestran una tendencia estacional a largo plazo (pico en octubre, caída en invierno, recuperación en primavera). La estacionariedad implica que la media y varianza son relativamente constantes. 2.2.2 Diferenciación Dado que la prueba ADF confirma que la serie ya es estacionaria (valor p = 0.0345 &lt; 0.05), la diferenciación no es necesaria. 2.2.3 Transformaciones La serie original muestra alta variabilidad, con picos grandes (~800) y caídas a 23.02. Esto sugiere una varianza no constante, que podría beneficiarse de una transformación logarítmica para estabilizar la variabilidad. Se verifica si hay valores no positivos:: ## [1] 23.02 El valor mínimo es 23.02, un valor positivo y no cercano a 0. Por lo tanto, se puede aplicar una transformación logarítmica log⁡(y). La gráfica de la serie transformada muestra que los picos se han reducido en escala. Por ejemplo, los picos más altos, que antes alcanzaban 800, ahora están en el rango de log⁡(800)≈6.6, y las caídas a 23.02 ahora son log⁡(23.02)≈3.1. Esto indica que la transformación estabiliza la varianza, haciendo las fluctuaciones más uniformes a lo largo del tiempo. La transformación no elimina la tendencia estacional ni el ciclo semanal ya que la transformación logarítmica afecta principalmente la varianza, no la tendencia ni la estacionalidad. Aunque la serie es estacionaria sin transformación, esta transformación mejora la estabilidad de la serie, lo que puede facilitar el modelado posterior (por ejemplo, SARIMA) y puede mejorar la precisión de las predicciones. 2.2.4 Justificación Diferenciación: No es necesaria, ya que la serie es estacionaria según la prueba ADF (valor p = 0.0345 &lt; 0.05). Transformación logarítmica: Aunque no es estrictamente necesaria para la estacionariedad, la transformación log⁡(y) ayuda a estabilizar la varianza de la serie, que tiene alta variabilidad. 2.3 Conclusiones El análisis temporal de las ventas diarias de la máquina de café revela los siguientes hallazgos: Tendencia estacional a largo plazo: Las ventas crecen hacia octubre (pico de 500), caen en invierno (300), y se recuperan en primavera (~400). Ciclo semanal: Tanto la ACF como la descomposición STL confirman un ciclo de 7 días, indicando que las ventas varían según el día de la semana. Se puede analizar las ventas por día de la semana para identificar días de alta demanda (por ejemplo, fines de semana). Alta variabilidad residual: El residuo de la descomposición STL muestra fluctuaciones significativas (hasta ±300), lo que sugiere que las ventas tienen un componente impredecible. Esto podría deberse a eventos externos (festivos, promociones), que vale la pena investigar para mejorar las predicciones. Estacionariedad y transformaciones: La serie original es estacionaria (prueba ADF, valor p = 0.0345 &lt; 0.05), por lo que la diferenciación no es necesaria. La transformación log estabiliza la varianza, reduciendo la magnitud de las fluctuaciones (de un rango de 23.02 a 800 a ~3.14 a ~6.68 en la escala logarítmica). Aunque la transformación no elimina la tendencia estacional ni el ciclo semanal, mejora la estabilidad de la serie, lo que facilita el modelado posterior. Un modelo como SARIMA puede ser adecuado para capturar la estacionalidad semanal y la tendencia estacional a largo plazo. En resumen, las ventas de la máquina de café presentan patrones claros a nivel semanal y estacional. La serie es estacionaria, por lo que está lista para modelado sin diferenciación. La transformación log⁡(y) mejora la estabilidad de la varianza, haciendo la serie más adecuada para modelos como SARIMA, que pueden capturar el ciclo estacional semanal y la tendencia estacional. "],["métodos-de-holt-winters-y-suavizamiento-exponencial-suavizamiento-exponencial-triple.html", "Capítulo 3 Métodos de Holt-Winters y Suavizamiento Exponencial (Suavizamiento Exponencial Triple) 3.1 Suavizamiento Exponencial Simple 3.2 Holt-Winters Aditivo 3.3 Holt-Winters Multiplicativo 3.4 Comparación de Modelos Holt-Winters", " Capítulo 3 Métodos de Holt-Winters y Suavizamiento Exponencial (Suavizamiento Exponencial Triple) El método Holt-Winters modela promedio, tendencia y estacionalidad. Dado que la serie tiene un ciclo semanal y una tendencia estacional a largo plazo, se probaron los siguientes tipos de suavizamiento: exponencial simple, aditivo y multiplicativo. Se aplican los métodos de Holt-Winters para modelar y predecir las ventas diarias para los próximos 7 días (un ciclo semanal). 3.1 Suavizamiento Exponencial Simple El suavizamiento exponencial simple modela solo el promedio de la serie y no considera tendencia ni estacionalidad, por lo que no es ideal para la serie de tiempo que tiene ambos componentes. Se aplicó a la serie transformada log(y) para evaluar su comportamiento en una escala con varianza estabilizada. Figure 3.1: Suavizamiento Exponencial Simple Como el suavizamiento exponencial simple solo modela la tendencia de la serie (sin tendencia ni estacionalidad), las predicciones son esencialmente un promedio suavizado de los valores históricos. En la escala logarítmica, las predicciones se mantienen constantes en un rango de ~5.8 a ~6 (equivalente a ~300 a ~400 en la escala original), sin reflejar el ciclo semanal ni la tendencia estacional. La gráfica evidencia que este modelo no captura los patrones semanales (como mayores ventas los fines de semana) ni la tendencia estacional a largo plazo. 3.2 Holt-Winters Aditivo En el modelo aditivo, la estacionalidad tiene una amplitud constante, lo que es consistente con la componente estacional de la descomposición STL (oscilando entre -20 y 20). Figure 3.2: Holt-Winters Aditivo El pronóstico con este metodo refleja el ciclo semanal que tiene la serie. La prediccion tiene picos y caidas que coinciden con el compotamiento fluctuante de las ventas reales. Como el modelo es aditivo, la amplitud de las fluctuaciones estacionales es constante, lo que coincide con la descomposición STL (entre -20 y 20). Esto significa que los picos y caídas en las ventas tienen una diferencia fija en valor absoluto. Los intervalos de confianza son relativamente estrechos, lo que indica que el modelo tiene una buena certeza en sus predicciones, aunque la alta variabilidad residual (±300) podría hacer que las ventas reales se desvíen de estas predicciones. 3.3 Holt-Winters Multiplicativo En el modelo multiplicativo, la amplitud de la estacionalidad varía con la tendencia de la serie. Esto puede ser significativo cuando las fluctuaciones sean más grandes en periodos de ventas altas. Figure 3.3: Holt-Winters Multiplicativo Este pronóstico también refleja el ciclo semanal que tiene la serie, pero asume que la amplitud de la estacionalidad varía con la tendencia de la serie. Por ejemplo, si las ventas promedio aumentan, las diferencias entre días de alta y baja demanda (como fines de semana vs. días laborales) se amplifican. Los intervalos de confianza son similares a los del modelo aditivo, pero tiene una amplitud menor en este rango de tiempo posiblemente porque la tendencia es más estable y el enfoque multiplicativo de variabilidad. 3.4 Comparación de Modelos Holt-Winters Se compara la precisión de los modelos aditivo y multiplicativo usando el error cuadrático medio (RMSE) y el error absoluto medio (MAE): ## RMSE MAE ## Additive 140.4213 111.8802 ## Multiplicative 143.8373 115.8820 El modelo Holt-Winters Aditivo es mejor, ya que tiene un RMSE y un MAE más bajos en comparación con el modelo Holt-Winters Multiplicativo. Esto indica que el modelo aditivo tiene un mejor ajuste a los datos y comete errores de predicción más pequeños en promedio. Además, la descomposición STL mostró que la estacionalidad de las ventas tiene una amplitud constante, lo que favorece al modelo aditivo, ya que este asume una estacionalidad constante, mientras que el modelo multiplicativo asume que la estacionalidad varía con el nivel de la serie. Dado que las ventas en ciertos meses presentan variaciones más grandes afectan la tendencia y generar diferencias entre los modelos, por esto el modelo aditivo es más adecuado. En las gráficas de pronóstico, los intervalos de confianza al 95% del método Holt-Winters Multiplicativo presentan una amplitud menor en comparación con los del método Aditivo. Esto indica que el modelo multiplicativo estima una menor incertidumbre en sus predicciones, posiblemente debido a que la tendencia de la serie es relativamente estable en este periodo. En contraste, el modelo aditivo, al asumir una estacionalidad constante, genera intervalos más amplios. RMSE (Root Mean Squared Error): El RMSE mide la raíz del promedio de los errores al cuadrado. Esta medida Penaliza más los errores grandes al elevarlos al cuadrado, lo que lo hace sensible a valores atípicos. MAE (Mean Absolute Error): El MAE mide el promedio de los errores absolutos. Representa el error promedio de las predicciones de manera directa, sin dar mayor peso a errores grandes, lo que lo hace más robusto frente a valores atípicos. La diferencia entre los modelos es de 3.416 en RMSE (2.43%) y 4.0018 en MAE (3.58%). Esta diferencia no se considera significativa, ya que es relativamente pequeña y en valor se diferencia entre 3-4 dólares frente a ventas promedio de 400 y una variabilidad residual de ±300. Dado que la diferencia es insignificante, ambos modelos tienen un rendimiento similar, aunque el aditivo sigue siendo preferible por su mejor ajuste y consistencia teórica con la estacionalidad constante. "],["ajuste-a-un-modelo-lineal-y-estacionario-modelo-arima.html", "Capítulo 4 Ajuste a un modelo lineal y estacionario (Modelo Arima) 4.1 Extensión de variables temporales 4.2 Modelo lineal 4.3 Modelo ARIMA estacional (SARIMA) 4.4 Comparación de Modelos", " Capítulo 4 Ajuste a un modelo lineal y estacionario (Modelo Arima) 4.1 Extensión de variables temporales Se va a enriquecer la fuente de datos con información detallada de la serie de tiempo, agregando el día de la semana (lunes, martes, etc.), el mes del año a cada registro diario. 4.2 Modelo lineal Se crea un modelo de regresión lineal para predecir las ventas diarias (valor_total) utilizando las variables temporales creadas (dia_semana, mes). ## ## Call: ## lm(formula = valor_total ~ dia_semana + mes, data = ventas_diarias) ## ## Residuals: ## Min 1Q Median 3Q Max ## -320.25 -98.99 -3.92 89.29 413.61 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 166.4158 31.1736 5.338 1.65e-07 *** ## dia_semanaMon 52.6861 26.5030 1.988 0.04756 * ## dia_semanaTue 66.7581 26.5110 2.518 0.01222 * ## dia_semanaWed 59.2610 26.5358 2.233 0.02613 * ## dia_semanaThu 24.8365 26.5358 0.936 0.34991 ## dia_semanaFri 77.3926 26.4103 2.930 0.00360 ** ## dia_semanaSat 33.9648 26.4935 1.282 0.20065 ## mesFeb 262.6093 36.3307 7.228 2.86e-12 *** ## mesMar 105.0106 31.5160 3.332 0.00095 *** ## mesApr 9.0262 35.7133 0.253 0.80061 ## mesMay 94.8890 35.9921 2.636 0.00874 ** ## mesJun 54.8875 35.7246 1.536 0.12530 ## mesJul 0.8016 35.4178 0.023 0.98196 ## mesAug 44.7058 35.4074 1.263 0.20753 ## mesSep 115.4321 35.7245 3.231 0.00134 ** ## mesOct 243.3142 35.4072 6.872 2.73e-11 *** ## mesNov 73.2084 35.7026 2.051 0.04102 * ## mesDec 53.5210 35.4393 1.510 0.13185 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 138.2 on 367 degrees of freedom ## Multiple R-squared: 0.2743, Adjusted R-squared: 0.2407 ## F-statistic: 8.159 on 17 and 367 DF, p-value: &lt; 2.2e-16 El modelo tiene coeficientes positivos para todos los días y meses, hay algunos días y meses que son significativos, esto es consistente con el comportamiento que se espera de las ventas, pero el R cuadrado ajustado es de solo 0.2407, Esto indica que el modelo explica ~24% de la variabilidad en las ventas. Esto es un resultado muy bajo pero consistente con la alta variabilidad residual (±300) observada en la descomposición STL, que no puede ser capturada completamente por las variables temporales. 4.3 Modelo ARIMA estacional (SARIMA) Dado que la serie es estacionaria (prueba ADF, valor p = 0.0345 &lt; 0.05), se va ajustar un modelo SARIMA para las ventas con el ciclo semanal. ## Series: ts_ventas ## ARIMA(1,0,1)(0,1,1)[7] ## ## Coefficients: ## ar1 ma1 sma1 ## 0.9456 -0.7941 -0.8238 ## s.e. 0.0271 0.0462 0.0438 ## ## sigma^2 = 19476: log likelihood = -2405.17 ## AIC=4818.34 AICc=4818.44 BIC=4834.08 ## ## Training set error measures: ## ME RMSE MAE MPE MAPE MASE ACF1 ## Training set 4.685073 137.7336 109.1637 -33.30582 59.42362 0.8025019 -0.0478096 4.3.1 Estructura del modelo El modelo ARIMA tiene una estructura (1,0,1)(0,1,1)[7]. Los componentes no estacionales (1,0,1) son: p = 1: Componente autorregresivo (AR) de orden 1. Esto confirma que las ventas actuales dependen linealmente de las ventas del día anterior como se vio en la gráfica ACF. d = 0: No se aplica diferenciación no estacional, lo cual es consistente con el resultado de la prueba ADF (valor p = 0.0345 &lt; 0.05), que indicó que la serie es estacionaria. q = 1: Componente de media móvil (MA) de orden 1. Significa que las ventas actuales dependen de los errores de predicción del día anterior. Los componentes estacionales (0,1,1) son: P = 0: No hay componente autorregresivo estacional. D = 1: Se aplica una diferenciación estacional de orden 1 para eliminar la estacionalidad semanal. Esto significa que el modelo trabaja con las diferencias de las ventas de cada día con el mismo día de la semana anterior (por ejemplo, ventas de un lunes menos las ventas del lunes anterior). Q = 1: Componente de media móvil estacional (SMA) de orden 1. Significa que los errores de predicción a nivel estacional también influyen en las ventas. s = 7: Frecuencia estacional de 7 días, coincide con los ciclos identificados en la gráfica ACF y la descomposición STL. Este modelo SARIMA captura tanto la dependencia a corto plazo (AR(1) y MA(1)) como el ciclo semanal (diferenciación estacional y SMA(1)). Que no haya diferenciación no estacional (d = 0) es consistente, ya que la serie es estacionaria, y la diferenciación estacional (D = 1) elimina el patrón semanal. 4.3.2 Coeficientes del modelo autorregresivo no estacional AR(1): ar1 = 0.9456 (s.e. = 0.0271): El valor de 0.9456 (cercano a 1) indica una fuerte autocorrelación en las ventas, las ventas de un día están altamente correlacionadas con las del día anterior. Esto es consistente con la ACF, que mostró autocorrelaciones significativas en los primeros rezagos (~0.3 a 0.4). El valor del error estándar es 0.0271, al calcular el estadístico t (t = 0.9456 / 0.0271 ≈ 34.89) da 34.89, este valor es mucho mayor a 1.96, por lo que el coeficiente es estadísticamente significativo con un p-valor cercano a 0. media móvil no estacional MA(1) ma1 = -0.7941 (s.e. = 0.0462): El valor negativo indica que los errores de predicción del día anterior tienen un efecto correctivo en las ventas actuales. El error estándar es 0.0462, al calcular el estadístico t da -17.19, su valor absoluto es mucho mayor a 1.96, que lo hace también significativo. media móvil estacional SMA(1) sma1 = -0.8238 (s.e. = 0.0438): El valor de -0.8238 indica que los errores de predicción a estacionales también tienen un efecto correctivo ayudando a modelar el ciclo semanal. El error estándar es 0.0438, su estadístico t es -18.81 por lo que también es significativo. Todos los coeficientes fueron significativos. El término AR(1) captura la dependencia a corto plazo, mientras que el término SMA(1) modela el ciclo semanal. Los términos MA(1) y SMA(1) ayudan a corregir los errores de predicción, mejorando el ajuste del modelo. 4.3.3 Varianza del error y Criterios de información Varianza del error sigma^2 = 19476: Esta es la varianza estimada de los errores del modelo en dolares. La raíz cuadrada de esta varianza es 139.56, este valor es una estimación de la desviación estándar de los errores. log likelihood = -2405.17: Este es el valor mide cómo el modelo se ajusta a los datos. Se usa para calcular los criterios de información (AIC, AICc, BIC). Criterios de información: AIC = 4818.34: Criterio de información de Akaike. Mide el ajuste del modelo penalizando la complejidad. Un valor más bajo indica un mejor modelo. AICc = 4818.44: Versión corregida del AIC para muestras pequeñas. BIC = 4834.08: Criterio de información bayesiano. Mide el ajuste del modelo penalizando más fuertemente la complejidad que el AIC. Estos valores no son directamente interpretables sin comparar con otros modelos SARIMA. 4.3.4 Métricas de error ME (Error Medio) = 4.685073: El error medio indica el sesgo promedio de las predicciones. Un valor de 4.68 sugiere en promedio el modelo tiende a predecir 4.68 unidades más de lo real. Esto es un sesgo pequeño comparando el rango de las ventas que varían entre 23.02 y 800. RMSE (Raíz del Error Cuadrático Medio) = 137.7336: El RMSE mide el error promedio de las ventas en las unidades (dolares) de la serie. Un valor de 137.73 indica que las predicciones del modelo se desvían en promedio en ~137.73 dolares de las ventas reales que es un valor alto pero explicado por la variabilidad residual (±300). MAE (Error Absoluto Medio) = 109.1637: El MAE mide el error promedio en valor absoluto. Un valor de 109.16 indica que, en promedio, las predicciones del modelo se desvían 109.16 unidades de las ventas reales. MPE (Error Porcentual Medio) = -33.30582%: El MPE mide el error porcentual promedio. El valor negativo indica que el modelo tiende a subestimar las ventas en promedio, lo que contradice el ME positivo. Esto puede pasar porque el MPE es sensible a errores relativos con valores pequeños (por ejemplo, días con ventas bajas como 23.02), donde un pequeño error absoluto puede traducirse en un gran error porcentual. MAPE (Error Porcentual Absoluto Medio) = 59.42362%: El MAPE mide el error porcentual absoluto promedio. Un valor de 59.42% indica que, en promedio, las predicciones del modelo se desvían en ~59.42% del valor real. Este valor es alto debido a la alta variabilidad de las ventas (de 23.02 a 800). Igual que con el MPE el MAPE tiende a ser grande cuando hay valores pequeños en la serie porque un error pequeño en términos absolutos resulta en un error porcentual grande. MASE (Error Absoluto Medio Escala) = 0.8025019: El MASE compara el MAE del modelo con el MAE de un modelo ingenuo. Un valor menor a 1 indica que el modelo SARIMA es mejor que el modelo ingenuo. ACF1 (Autocorrelación del primer rezago de los residuales) = -0.0478096: Este valor mide la autocorrelación de los residuales en el primer rezago. Un valor cercano a 0 indica que los residuales del modelo no tienen autocorrelación significativa. Significa que el modelo modela la mayor parte de la estructura temporal de la serie dejando residuales que se comportan como ruido blanco. 4.4 Comparación de Modelos Se comparamos la precisión de los modelos Holt-Winters (Aditivo y Multiplicativo), el modelo lineal, y el modelo SARIMA usando el error cuadrático medio (RMSE) y el error absoluto medio (MAE). ## RMSE MAE ## Additive 140.4213 111.8802 ## Multiplicative 143.8373 115.8820 ## Linear 134.9217 108.0084 ## SARIMA 137.7336 109.1637 El SARIMA tiene el mejor desempeño en términos de RMSE y MAE, lo que sugiere que es el mejor modelo hasta ahora. Esto puede deberse a que el SARIMA modela la estructura autorregresiva y la estacionalidad semanal, mientras que Holt-Winters se basa en suavizamiento y el modelo lineal no captura la estructura temporal. "],["algoritmo-facebooks-prophet.html", "Capítulo 5 Algoritmo Facebook´s Prophet 5.1 Componentes 5.2 Justificación", " Capítulo 5 Algoritmo Facebook´s Prophet 5.1 Componentes Prophet está basado en un modelo aditivo que descompone las series temporal en tres componentes principales: y(t)=g(t)+s(t)+h(t)+ϵt y(t): Variable dependiente a modelar. g(t): Es la tendencia que puede ser lineal o logística. Esta tendencia tiene puntos de cambio automáticos o definidos por el usuario. s(t): Estacionalidad, modelada mediante series de Fourier para capturar patrones periódicos (diarios, semanales, anuales). h(t): Efectos de días festivos o eventos especiales, que pueden ser especificados manualmente. ϵt: Error asumido como ruido blanco. Prophet es muy útil para series temporales con fuerte estacionalidad, datos históricos con patrones estacionales y efectos de eventos externos, como festivos, que impactan las ventas. También es robusto ante valores faltantes, ruido o outliers. 5.1.1 Modelamiento Se crea el modelo usando el algoritmo Prophet sin regresores. ## ds yhat yhat_lower yhat_upper ## 379 2025-03-17 379.3272 190.3931 569.6825 ## 380 2025-03-18 395.2764 206.9502 595.4971 ## 381 2025-03-19 382.8862 202.2388 570.4434 ## 382 2025-03-20 353.0883 154.3049 538.4603 ## 383 2025-03-21 400.6356 212.4587 592.5257 ## 384 2025-03-22 363.9774 171.1671 559.7801 ## 385 2025-03-23 334.8151 141.7672 528.9348 En este caso se modelaron los 7 días siguientes de la venta, los componentes del dataframe resultante son: ds: Fecha. yhat: Valor predicho. yhat_lower: Límite inferior al 80% de confianza. yhat_upper: Límite superior al 80% de confianza. En esta grafica se puede ver que el modelo solo fue capaz de modelar 3 de los 7 puntos cercanos a los valores reales, esto se puede explicar por la alta variabilidad residual (±300) observada en la descomposición STL. Además, puede que los componentes principales (tendencia, estacionalidad y días especiales) estén subestimando sus efectos en el tiempo. 5.1.2 Métricas Se calcula el RMSE y MAE del modelo: ## RMSE Prophet: 196.8354 ## MAE Prophet: 146.5227 Estos resultados son los peores de todos los modelos que se han generado, el resultado es consistente con las conclusiones del analisis de la grafica anterior. 5.1.3 Regresores En Prophet, los regresores son variables externas que se pueden añadir al modelo para mejorar las predicciones. Estas variables se suman al modelo original para capturar efectos que no se explican completamente por la tendencia, la estacionalidad o los festivos. Estas variables se incorporan como términos lineales en la ecuación, quedando: y(t)=g(t)+s(t)+h(t)+β1x1(t)+β2x2(t)+⋯+βnxn(t)+ϵty(t) = g Donde: x1(t),x2(t),…,xn(t): Son los regresores en el tiempo. β1,β2,…,βn: Son los coeficientes que Prophet estima para cada regresor. En tu caso, los regresores que se usarán son dia_semana y mes para capturar mejor los efectos de los días de la semana y los meses del año, buscando un mejor resultado ## RMSE Prophet con regresores: 176.8185 ## MAE Prophet con regresores: 134.5342 La inclusión de regresores mejoró el desempeño de Prophet, reduciendo el RMSE en un 10.2% y el MAE en un 8.2%. Esto confirma que los regresores ayudan a capturar efectos específicos haciendo que el modelo sea más preciso, pero todavía no alcanza el nivel de SARIMA o Holt-Winters. 5.2 Justificación La serie de tiempo que se esta estudiando es numérica y continua, lo cual es típico en problemas de regresión y puede ser tratada como un problema de regresión porque Prophet modela la variable dependiente y(t) = ventas como una función de tiempo, descompuesta en componentes aditivos de tendencia, estacionalidad y días especiales. Los componentes aditivos de Prophet permite interpretar los efectos de cada componente, similar a cómo se interpretan los coeficientes en una regresión. Este enfoque es conceptualmente similar a una regresión no lineal, donde el tiempo actúa como un regresor y los componentes de tendencia y estacionalidad son funciones del tiempo ajustadas al comportamiento de los datos. Además: Sin embargo, Prophet no modela explícitamente la dependencia temporal autorregresiva (como en SARIMA), sino que enfoca el problema como una regresión basada en componentes del tiempo. Esto lo hace más interpretable y fácil de usar, pero puede limitar su capacidad para capturar dinámicas autorregresivas complejas, como se vió en el ACF. "],["redes-neuronales-elman-y-jordan.html", "Capítulo 6 Redes neuronales ELMAN y Jordan 6.1 ELMAN 6.2 Jordan 6.3 Resultados", " Capítulo 6 Redes neuronales ELMAN y Jordan En este capítulo se van a aplicar los modelos de redes neuronales ELMAN y Jordan comparando sus resultados contra los modelos anteriores para buscar cual es más adecuado para predecir las ventas. Se inicia la preparación de los datos completando los días faltantes en la base de datos. Estos valores faltantes pueden ser días que no hubo venta o que no se subieron al repositorio, como no se tiene claro se van a agregar y se van a imputar sus valores usando interpolación lineal. Esto se hace porque estas redes neuronales son modelos recurrentes y dependen de una secuencia temporal continua para capturar dependencias temporales. Como las ventas presentan un comportamiento cíclico semanal se van a crear variables desfasadas con un valor de 7 para tener en cuenta esta cantidad de días en el modelo. Después se separan los conjuntos de entrenamiento y prueba y se normalizan debido a que las redes neuronales son sensibles a las escalas de los valores. 6.1 ELMAN Estructura: Es una red recurrente con una capa de entrada, una capa oculta, una capa de salida y una capa de contexto. La capa de contexto almacena las activaciones de las unidades ocultas del paso anterior y las retroalimenta como entrada adicional en el paso siguiente. Figure 6.1: Estrutura red ELMAN El modelo se entrenó con los siguientes parámetros usando la librería keras: units = 10 Este parámetro indica el número de neuronas de las capas ocultas y de contexto. Esta librería gestiona ambas capas automáticamente. epochs = 250 Este parámetro indica la cantidad de veces que el modelo repasa el conjunto de entrenamiento. Cada vez que esto ocurre se calculan nuevos gradientes para ajustar los pesos de la red neuronal para reducir la perdida. batch_size = 2 Indica el número de muestras procesadas antes de actualizar los pesos en cada iteración. validation_split = 0.2: Con este parámetro se reserva el 20% de los datos de entrenamiento para validar el modelo durante el entrenamiento para evitar el sobreajuste. ## Epoch 1/250 ## 150/150 - 1s - loss: 0.0823 - val_loss: 0.0641 - 1s/epoch - 10ms/step ## Epoch 2/250 ## 150/150 - 0s - loss: 0.0368 - val_loss: 0.0488 - 327ms/epoch - 2ms/step ## Epoch 3/250 ## 150/150 - 0s - loss: 0.0313 - val_loss: 0.0522 - 331ms/epoch - 2ms/step ## Epoch 4/250 ## 150/150 - 0s - loss: 0.0300 - val_loss: 0.0438 - 307ms/epoch - 2ms/step ## Epoch 5/250 ## 150/150 - 0s - loss: 0.0279 - val_loss: 0.0404 - 304ms/epoch - 2ms/step ## Epoch 6/250 ## 150/150 - 0s - loss: 0.0276 - val_loss: 0.0436 - 308ms/epoch - 2ms/step ## Epoch 7/250 ## 150/150 - 0s - loss: 0.0269 - val_loss: 0.0458 - 308ms/epoch - 2ms/step ## Epoch 8/250 ## 150/150 - 0s - loss: 0.0267 - val_loss: 0.0409 - 305ms/epoch - 2ms/step ## Epoch 9/250 ## 150/150 - 0s - loss: 0.0269 - val_loss: 0.0392 - 320ms/epoch - 2ms/step ## Epoch 10/250 ## 150/150 - 0s - loss: 0.0270 - val_loss: 0.0370 - 302ms/epoch - 2ms/step ## Epoch 11/250 ## 150/150 - 0s - loss: 0.0263 - val_loss: 0.0416 - 318ms/epoch - 2ms/step ## Epoch 12/250 ## 150/150 - 0s - loss: 0.0273 - val_loss: 0.0373 - 313ms/epoch - 2ms/step ## Epoch 13/250 ## 150/150 - 0s - loss: 0.0267 - val_loss: 0.0407 - 298ms/epoch - 2ms/step ## Epoch 14/250 ## 150/150 - 0s - loss: 0.0262 - val_loss: 0.0413 - 313ms/epoch - 2ms/step ## Epoch 15/250 ## 150/150 - 0s - loss: 0.0272 - val_loss: 0.0374 - 348ms/epoch - 2ms/step ## Epoch 16/250 ## 150/150 - 0s - loss: 0.0269 - val_loss: 0.0375 - 311ms/epoch - 2ms/step ## Epoch 17/250 ## 150/150 - 0s - loss: 0.0261 - val_loss: 0.0376 - 332ms/epoch - 2ms/step ## Epoch 18/250 ## 150/150 - 0s - loss: 0.0265 - val_loss: 0.0385 - 340ms/epoch - 2ms/step ## Epoch 19/250 ## 150/150 - 0s - loss: 0.0270 - val_loss: 0.0394 - 427ms/epoch - 3ms/step ## Epoch 20/250 ## 150/150 - 0s - loss: 0.0268 - val_loss: 0.0381 - 337ms/epoch - 2ms/step ## Epoch 21/250 ## 150/150 - 0s - loss: 0.0260 - val_loss: 0.0432 - 305ms/epoch - 2ms/step ## Epoch 22/250 ## 150/150 - 0s - loss: 0.0263 - val_loss: 0.0367 - 325ms/epoch - 2ms/step ## Epoch 23/250 ## 150/150 - 0s - loss: 0.0270 - val_loss: 0.0380 - 339ms/epoch - 2ms/step ## Epoch 24/250 ## 150/150 - 0s - loss: 0.0266 - val_loss: 0.0371 - 354ms/epoch - 2ms/step ## Epoch 25/250 ## 150/150 - 0s - loss: 0.0266 - val_loss: 0.0387 - 331ms/epoch - 2ms/step ## Epoch 26/250 ## 150/150 - 0s - loss: 0.0262 - val_loss: 0.0366 - 308ms/epoch - 2ms/step ## Epoch 27/250 ## 150/150 - 0s - loss: 0.0260 - val_loss: 0.0362 - 310ms/epoch - 2ms/step ## Epoch 28/250 ## 150/150 - 0s - loss: 0.0263 - val_loss: 0.0368 - 315ms/epoch - 2ms/step ## Epoch 29/250 ## 150/150 - 0s - loss: 0.0256 - val_loss: 0.0367 - 312ms/epoch - 2ms/step ## Epoch 30/250 ## 150/150 - 0s - loss: 0.0265 - val_loss: 0.0382 - 305ms/epoch - 2ms/step ## Epoch 31/250 ## 150/150 - 0s - loss: 0.0263 - val_loss: 0.0392 - 309ms/epoch - 2ms/step ## Epoch 32/250 ## 150/150 - 0s - loss: 0.0260 - val_loss: 0.0379 - 324ms/epoch - 2ms/step ## Epoch 33/250 ## 150/150 - 0s - loss: 0.0262 - val_loss: 0.0387 - 359ms/epoch - 2ms/step ## Epoch 34/250 ## 150/150 - 0s - loss: 0.0259 - val_loss: 0.0368 - 347ms/epoch - 2ms/step ## Epoch 35/250 ## 150/150 - 0s - loss: 0.0262 - val_loss: 0.0372 - 387ms/epoch - 3ms/step ## Epoch 36/250 ## 150/150 - 0s - loss: 0.0268 - val_loss: 0.0371 - 404ms/epoch - 3ms/step ## Epoch 37/250 ## 150/150 - 0s - loss: 0.0260 - val_loss: 0.0399 - 372ms/epoch - 2ms/step ## Epoch 38/250 ## 150/150 - 0s - loss: 0.0263 - val_loss: 0.0390 - 333ms/epoch - 2ms/step ## Epoch 39/250 ## 150/150 - 0s - loss: 0.0270 - val_loss: 0.0382 - 327ms/epoch - 2ms/step ## Epoch 40/250 ## 150/150 - 0s - loss: 0.0257 - val_loss: 0.0375 - 396ms/epoch - 3ms/step ## Epoch 41/250 ## 150/150 - 0s - loss: 0.0263 - val_loss: 0.0376 - 326ms/epoch - 2ms/step ## Epoch 42/250 ## 150/150 - 0s - loss: 0.0260 - val_loss: 0.0407 - 350ms/epoch - 2ms/step ## Epoch 43/250 ## 150/150 - 0s - loss: 0.0259 - val_loss: 0.0471 - 402ms/epoch - 3ms/step ## Epoch 44/250 ## 150/150 - 0s - loss: 0.0266 - val_loss: 0.0393 - 346ms/epoch - 2ms/step ## Epoch 45/250 ## 150/150 - 1s - loss: 0.0264 - val_loss: 0.0360 - 563ms/epoch - 4ms/step ## Epoch 46/250 ## 150/150 - 0s - loss: 0.0261 - val_loss: 0.0363 - 494ms/epoch - 3ms/step ## Epoch 47/250 ## 150/150 - 0s - loss: 0.0263 - val_loss: 0.0399 - 473ms/epoch - 3ms/step ## Epoch 48/250 ## 150/150 - 0s - loss: 0.0264 - val_loss: 0.0392 - 500ms/epoch - 3ms/step ## Epoch 49/250 ## 150/150 - 0s - loss: 0.0260 - val_loss: 0.0375 - 420ms/epoch - 3ms/step ## Epoch 50/250 ## 150/150 - 0s - loss: 0.0260 - val_loss: 0.0375 - 379ms/epoch - 3ms/step ## Epoch 51/250 ## 150/150 - 1s - loss: 0.0256 - val_loss: 0.0375 - 568ms/epoch - 4ms/step ## Epoch 52/250 ## 150/150 - 0s - loss: 0.0261 - val_loss: 0.0362 - 346ms/epoch - 2ms/step ## Epoch 53/250 ## 150/150 - 0s - loss: 0.0265 - val_loss: 0.0370 - 361ms/epoch - 2ms/step ## Epoch 54/250 ## 150/150 - 0s - loss: 0.0262 - val_loss: 0.0374 - 340ms/epoch - 2ms/step ## Epoch 55/250 ## 150/150 - 0s - loss: 0.0264 - val_loss: 0.0362 - 328ms/epoch - 2ms/step ## Epoch 56/250 ## 150/150 - 0s - loss: 0.0265 - val_loss: 0.0369 - 314ms/epoch - 2ms/step ## Epoch 57/250 ## 150/150 - 0s - loss: 0.0260 - val_loss: 0.0377 - 317ms/epoch - 2ms/step ## Epoch 58/250 ## 150/150 - 0s - loss: 0.0258 - val_loss: 0.0378 - 305ms/epoch - 2ms/step ## Epoch 59/250 ## 150/150 - 0s - loss: 0.0259 - val_loss: 0.0380 - 296ms/epoch - 2ms/step ## Epoch 60/250 ## 150/150 - 0s - loss: 0.0261 - val_loss: 0.0383 - 302ms/epoch - 2ms/step ## Epoch 61/250 ## 150/150 - 0s - loss: 0.0262 - val_loss: 0.0407 - 295ms/epoch - 2ms/step ## Epoch 62/250 ## 150/150 - 0s - loss: 0.0259 - val_loss: 0.0363 - 413ms/epoch - 3ms/step ## Epoch 63/250 ## 150/150 - 0s - loss: 0.0259 - val_loss: 0.0357 - 379ms/epoch - 3ms/step ## Epoch 64/250 ## 150/150 - 0s - loss: 0.0261 - val_loss: 0.0363 - 292ms/epoch - 2ms/step ## Epoch 65/250 ## 150/150 - 0s - loss: 0.0255 - val_loss: 0.0358 - 306ms/epoch - 2ms/step ## Epoch 66/250 ## 150/150 - 0s - loss: 0.0260 - val_loss: 0.0388 - 297ms/epoch - 2ms/step ## Epoch 67/250 ## 150/150 - 0s - loss: 0.0258 - val_loss: 0.0369 - 287ms/epoch - 2ms/step ## Epoch 68/250 ## 150/150 - 0s - loss: 0.0261 - val_loss: 0.0393 - 307ms/epoch - 2ms/step ## Epoch 69/250 ## 150/150 - 0s - loss: 0.0262 - val_loss: 0.0385 - 287ms/epoch - 2ms/step ## Epoch 70/250 ## 150/150 - 0s - loss: 0.0260 - val_loss: 0.0364 - 297ms/epoch - 2ms/step ## Epoch 71/250 ## 150/150 - 0s - loss: 0.0258 - val_loss: 0.0417 - 287ms/epoch - 2ms/step ## Epoch 72/250 ## 150/150 - 0s - loss: 0.0268 - val_loss: 0.0359 - 288ms/epoch - 2ms/step ## Epoch 73/250 ## 150/150 - 0s - loss: 0.0257 - val_loss: 0.0359 - 297ms/epoch - 2ms/step ## Epoch 74/250 ## 150/150 - 0s - loss: 0.0261 - val_loss: 0.0386 - 292ms/epoch - 2ms/step ## Epoch 75/250 ## 150/150 - 0s - loss: 0.0261 - val_loss: 0.0380 - 291ms/epoch - 2ms/step ## Epoch 76/250 ## 150/150 - 0s - loss: 0.0258 - val_loss: 0.0372 - 290ms/epoch - 2ms/step ## Epoch 77/250 ## 150/150 - 0s - loss: 0.0256 - val_loss: 0.0372 - 288ms/epoch - 2ms/step ## Epoch 78/250 ## 150/150 - 0s - loss: 0.0260 - val_loss: 0.0352 - 292ms/epoch - 2ms/step ## Epoch 79/250 ## 150/150 - 0s - loss: 0.0256 - val_loss: 0.0362 - 347ms/epoch - 2ms/step ## Epoch 80/250 ## 150/150 - 0s - loss: 0.0255 - val_loss: 0.0364 - 328ms/epoch - 2ms/step ## Epoch 81/250 ## 150/150 - 0s - loss: 0.0259 - val_loss: 0.0368 - 416ms/epoch - 3ms/step ## Epoch 82/250 ## 150/150 - 0s - loss: 0.0256 - val_loss: 0.0365 - 316ms/epoch - 2ms/step ## Epoch 83/250 ## 150/150 - 0s - loss: 0.0256 - val_loss: 0.0362 - 364ms/epoch - 2ms/step ## Epoch 84/250 ## 150/150 - 0s - loss: 0.0259 - val_loss: 0.0376 - 401ms/epoch - 3ms/step ## Epoch 85/250 ## 150/150 - 0s - loss: 0.0260 - val_loss: 0.0351 - 313ms/epoch - 2ms/step ## Epoch 86/250 ## 150/150 - 0s - loss: 0.0261 - val_loss: 0.0360 - 308ms/epoch - 2ms/step ## Epoch 87/250 ## 150/150 - 0s - loss: 0.0259 - val_loss: 0.0372 - 291ms/epoch - 2ms/step ## Epoch 88/250 ## 150/150 - 0s - loss: 0.0262 - val_loss: 0.0364 - 302ms/epoch - 2ms/step ## Epoch 89/250 ## 150/150 - 0s - loss: 0.0256 - val_loss: 0.0353 - 303ms/epoch - 2ms/step ## Epoch 90/250 ## 150/150 - 0s - loss: 0.0264 - val_loss: 0.0389 - 320ms/epoch - 2ms/step ## Epoch 91/250 ## 150/150 - 0s - loss: 0.0262 - val_loss: 0.0352 - 365ms/epoch - 2ms/step ## Epoch 92/250 ## 150/150 - 0s - loss: 0.0260 - val_loss: 0.0369 - 325ms/epoch - 2ms/step ## Epoch 93/250 ## 150/150 - 0s - loss: 0.0254 - val_loss: 0.0380 - 322ms/epoch - 2ms/step ## Epoch 94/250 ## 150/150 - 0s - loss: 0.0262 - val_loss: 0.0366 - 306ms/epoch - 2ms/step ## Epoch 95/250 ## 150/150 - 0s - loss: 0.0261 - val_loss: 0.0367 - 306ms/epoch - 2ms/step ## Epoch 96/250 ## 150/150 - 0s - loss: 0.0259 - val_loss: 0.0350 - 320ms/epoch - 2ms/step ## Epoch 97/250 ## 150/150 - 0s - loss: 0.0260 - val_loss: 0.0373 - 320ms/epoch - 2ms/step ## Epoch 98/250 ## 150/150 - 0s - loss: 0.0257 - val_loss: 0.0366 - 294ms/epoch - 2ms/step ## Epoch 99/250 ## 150/150 - 0s - loss: 0.0259 - val_loss: 0.0351 - 292ms/epoch - 2ms/step ## Epoch 100/250 ## 150/150 - 0s - loss: 0.0256 - val_loss: 0.0363 - 292ms/epoch - 2ms/step ## Epoch 101/250 ## 150/150 - 0s - loss: 0.0256 - val_loss: 0.0381 - 291ms/epoch - 2ms/step ## Epoch 102/250 ## 150/150 - 0s - loss: 0.0258 - val_loss: 0.0378 - 303ms/epoch - 2ms/step ## Epoch 103/250 ## 150/150 - 0s - loss: 0.0259 - val_loss: 0.0366 - 290ms/epoch - 2ms/step ## Epoch 104/250 ## 150/150 - 0s - loss: 0.0257 - val_loss: 0.0368 - 296ms/epoch - 2ms/step ## Epoch 105/250 ## 150/150 - 0s - loss: 0.0257 - val_loss: 0.0382 - 295ms/epoch - 2ms/step ## Epoch 106/250 ## 150/150 - 0s - loss: 0.0256 - val_loss: 0.0364 - 306ms/epoch - 2ms/step ## Epoch 107/250 ## 150/150 - 0s - loss: 0.0259 - val_loss: 0.0357 - 291ms/epoch - 2ms/step ## Epoch 108/250 ## 150/150 - 0s - loss: 0.0253 - val_loss: 0.0382 - 292ms/epoch - 2ms/step ## Epoch 109/250 ## 150/150 - 0s - loss: 0.0259 - val_loss: 0.0367 - 313ms/epoch - 2ms/step ## Epoch 110/250 ## 150/150 - 0s - loss: 0.0259 - val_loss: 0.0354 - 292ms/epoch - 2ms/step ## Epoch 111/250 ## 150/150 - 0s - loss: 0.0258 - val_loss: 0.0358 - 292ms/epoch - 2ms/step ## Epoch 112/250 ## 150/150 - 0s - loss: 0.0255 - val_loss: 0.0372 - 294ms/epoch - 2ms/step ## Epoch 113/250 ## 150/150 - 0s - loss: 0.0265 - val_loss: 0.0359 - 295ms/epoch - 2ms/step ## Epoch 114/250 ## 150/150 - 0s - loss: 0.0259 - val_loss: 0.0390 - 294ms/epoch - 2ms/step ## Epoch 115/250 ## 150/150 - 0s - loss: 0.0255 - val_loss: 0.0378 - 289ms/epoch - 2ms/step ## Epoch 116/250 ## 150/150 - 0s - loss: 0.0259 - val_loss: 0.0378 - 294ms/epoch - 2ms/step ## Epoch 117/250 ## 150/150 - 0s - loss: 0.0260 - val_loss: 0.0357 - 294ms/epoch - 2ms/step ## Epoch 118/250 ## 150/150 - 0s - loss: 0.0259 - val_loss: 0.0363 - 290ms/epoch - 2ms/step ## Epoch 119/250 ## 150/150 - 0s - loss: 0.0256 - val_loss: 0.0375 - 293ms/epoch - 2ms/step ## Epoch 120/250 ## 150/150 - 0s - loss: 0.0256 - val_loss: 0.0350 - 304ms/epoch - 2ms/step ## Epoch 121/250 ## 150/150 - 0s - loss: 0.0259 - val_loss: 0.0348 - 300ms/epoch - 2ms/step ## Epoch 122/250 ## 150/150 - 0s - loss: 0.0255 - val_loss: 0.0369 - 298ms/epoch - 2ms/step ## Epoch 123/250 ## 150/150 - 0s - loss: 0.0258 - val_loss: 0.0352 - 293ms/epoch - 2ms/step ## Epoch 124/250 ## 150/150 - 0s - loss: 0.0256 - val_loss: 0.0368 - 294ms/epoch - 2ms/step ## Epoch 125/250 ## 150/150 - 0s - loss: 0.0260 - val_loss: 0.0360 - 289ms/epoch - 2ms/step ## Epoch 126/250 ## 150/150 - 0s - loss: 0.0254 - val_loss: 0.0352 - 290ms/epoch - 2ms/step ## Epoch 127/250 ## 150/150 - 0s - loss: 0.0263 - val_loss: 0.0366 - 295ms/epoch - 2ms/step ## Epoch 128/250 ## 150/150 - 0s - loss: 0.0257 - val_loss: 0.0383 - 291ms/epoch - 2ms/step ## Epoch 129/250 ## 150/150 - 0s - loss: 0.0258 - val_loss: 0.0367 - 295ms/epoch - 2ms/step ## Epoch 130/250 ## 150/150 - 0s - loss: 0.0256 - val_loss: 0.0354 - 289ms/epoch - 2ms/step ## Epoch 131/250 ## 150/150 - 0s - loss: 0.0256 - val_loss: 0.0349 - 292ms/epoch - 2ms/step ## Epoch 132/250 ## 150/150 - 0s - loss: 0.0257 - val_loss: 0.0356 - 297ms/epoch - 2ms/step ## Epoch 133/250 ## 150/150 - 0s - loss: 0.0253 - val_loss: 0.0352 - 301ms/epoch - 2ms/step ## Epoch 134/250 ## 150/150 - 0s - loss: 0.0257 - val_loss: 0.0359 - 289ms/epoch - 2ms/step ## Epoch 135/250 ## 150/150 - 0s - loss: 0.0259 - val_loss: 0.0363 - 296ms/epoch - 2ms/step ## Epoch 136/250 ## 150/150 - 0s - loss: 0.0258 - val_loss: 0.0356 - 293ms/epoch - 2ms/step ## Epoch 137/250 ## 150/150 - 0s - loss: 0.0254 - val_loss: 0.0364 - 289ms/epoch - 2ms/step ## Epoch 138/250 ## 150/150 - 0s - loss: 0.0253 - val_loss: 0.0370 - 293ms/epoch - 2ms/step ## Epoch 139/250 ## 150/150 - 0s - loss: 0.0263 - val_loss: 0.0377 - 290ms/epoch - 2ms/step ## Epoch 140/250 ## 150/150 - 0s - loss: 0.0255 - val_loss: 0.0374 - 333ms/epoch - 2ms/step ## Epoch 141/250 ## 150/150 - 0s - loss: 0.0256 - val_loss: 0.0364 - 305ms/epoch - 2ms/step ## Epoch 142/250 ## 150/150 - 0s - loss: 0.0256 - val_loss: 0.0396 - 315ms/epoch - 2ms/step ## Epoch 143/250 ## 150/150 - 0s - loss: 0.0257 - val_loss: 0.0360 - 297ms/epoch - 2ms/step ## Epoch 144/250 ## 150/150 - 0s - loss: 0.0258 - val_loss: 0.0386 - 315ms/epoch - 2ms/step ## Epoch 145/250 ## 150/150 - 0s - loss: 0.0256 - val_loss: 0.0353 - 303ms/epoch - 2ms/step ## Epoch 146/250 ## 150/150 - 0s - loss: 0.0257 - val_loss: 0.0364 - 303ms/epoch - 2ms/step ## Epoch 147/250 ## 150/150 - 0s - loss: 0.0258 - val_loss: 0.0366 - 299ms/epoch - 2ms/step ## Epoch 148/250 ## 150/150 - 0s - loss: 0.0254 - val_loss: 0.0345 - 294ms/epoch - 2ms/step ## Epoch 149/250 ## 150/150 - 0s - loss: 0.0257 - val_loss: 0.0358 - 319ms/epoch - 2ms/step ## Epoch 150/250 ## 150/150 - 0s - loss: 0.0256 - val_loss: 0.0344 - 339ms/epoch - 2ms/step ## Epoch 151/250 ## 150/150 - 0s - loss: 0.0261 - val_loss: 0.0361 - 385ms/epoch - 3ms/step ## Epoch 152/250 ## 150/150 - 0s - loss: 0.0256 - val_loss: 0.0370 - 410ms/epoch - 3ms/step ## Epoch 153/250 ## 150/150 - 0s - loss: 0.0253 - val_loss: 0.0392 - 360ms/epoch - 2ms/step ## Epoch 154/250 ## 150/150 - 0s - loss: 0.0263 - val_loss: 0.0360 - 328ms/epoch - 2ms/step ## Epoch 155/250 ## 150/150 - 0s - loss: 0.0255 - val_loss: 0.0362 - 308ms/epoch - 2ms/step ## Epoch 156/250 ## 150/150 - 0s - loss: 0.0254 - val_loss: 0.0351 - 314ms/epoch - 2ms/step ## Epoch 157/250 ## 150/150 - 0s - loss: 0.0255 - val_loss: 0.0372 - 284ms/epoch - 2ms/step ## Epoch 158/250 ## 150/150 - 0s - loss: 0.0261 - val_loss: 0.0358 - 289ms/epoch - 2ms/step ## Epoch 159/250 ## 150/150 - 0s - loss: 0.0256 - val_loss: 0.0376 - 342ms/epoch - 2ms/step ## Epoch 160/250 ## 150/150 - 0s - loss: 0.0257 - val_loss: 0.0375 - 439ms/epoch - 3ms/step ## Epoch 161/250 ## 150/150 - 0s - loss: 0.0253 - val_loss: 0.0376 - 301ms/epoch - 2ms/step ## Epoch 162/250 ## 150/150 - 0s - loss: 0.0257 - val_loss: 0.0362 - 303ms/epoch - 2ms/step ## Epoch 163/250 ## 150/150 - 0s - loss: 0.0253 - val_loss: 0.0355 - 289ms/epoch - 2ms/step ## Epoch 164/250 ## 150/150 - 0s - loss: 0.0251 - val_loss: 0.0382 - 296ms/epoch - 2ms/step ## Epoch 165/250 ## 150/150 - 0s - loss: 0.0258 - val_loss: 0.0348 - 298ms/epoch - 2ms/step ## Epoch 166/250 ## 150/150 - 0s - loss: 0.0259 - val_loss: 0.0354 - 294ms/epoch - 2ms/step ## Epoch 167/250 ## 150/150 - 0s - loss: 0.0260 - val_loss: 0.0353 - 341ms/epoch - 2ms/step ## Epoch 168/250 ## 150/150 - 0s - loss: 0.0253 - val_loss: 0.0344 - 316ms/epoch - 2ms/step ## Epoch 169/250 ## 150/150 - 0s - loss: 0.0258 - val_loss: 0.0376 - 295ms/epoch - 2ms/step ## Epoch 170/250 ## 150/150 - 0s - loss: 0.0255 - val_loss: 0.0363 - 315ms/epoch - 2ms/step ## Epoch 171/250 ## 150/150 - 0s - loss: 0.0257 - val_loss: 0.0353 - 317ms/epoch - 2ms/step ## Epoch 172/250 ## 150/150 - 0s - loss: 0.0254 - val_loss: 0.0345 - 306ms/epoch - 2ms/step ## Epoch 173/250 ## 150/150 - 0s - loss: 0.0257 - val_loss: 0.0346 - 317ms/epoch - 2ms/step ## Epoch 174/250 ## 150/150 - 0s - loss: 0.0254 - val_loss: 0.0372 - 321ms/epoch - 2ms/step ## Epoch 175/250 ## 150/150 - 0s - loss: 0.0255 - val_loss: 0.0343 - 293ms/epoch - 2ms/step ## Epoch 176/250 ## 150/150 - 0s - loss: 0.0252 - val_loss: 0.0362 - 311ms/epoch - 2ms/step ## Epoch 177/250 ## 150/150 - 0s - loss: 0.0255 - val_loss: 0.0360 - 318ms/epoch - 2ms/step ## Epoch 178/250 ## 150/150 - 0s - loss: 0.0252 - val_loss: 0.0371 - 316ms/epoch - 2ms/step ## Epoch 179/250 ## 150/150 - 0s - loss: 0.0257 - val_loss: 0.0346 - 297ms/epoch - 2ms/step ## Epoch 180/250 ## 150/150 - 0s - loss: 0.0254 - val_loss: 0.0363 - 324ms/epoch - 2ms/step ## Epoch 181/250 ## 150/150 - 0s - loss: 0.0254 - val_loss: 0.0349 - 298ms/epoch - 2ms/step ## Epoch 182/250 ## 150/150 - 0s - loss: 0.0256 - val_loss: 0.0384 - 289ms/epoch - 2ms/step ## Epoch 183/250 ## 150/150 - 0s - loss: 0.0254 - val_loss: 0.0356 - 292ms/epoch - 2ms/step ## Epoch 184/250 ## 150/150 - 0s - loss: 0.0254 - val_loss: 0.0347 - 319ms/epoch - 2ms/step ## Epoch 185/250 ## 150/150 - 0s - loss: 0.0254 - val_loss: 0.0358 - 302ms/epoch - 2ms/step ## Epoch 186/250 ## 150/150 - 0s - loss: 0.0254 - val_loss: 0.0368 - 312ms/epoch - 2ms/step ## Epoch 187/250 ## 150/150 - 0s - loss: 0.0257 - val_loss: 0.0353 - 363ms/epoch - 2ms/step ## Epoch 188/250 ## 150/150 - 0s - loss: 0.0256 - val_loss: 0.0367 - 302ms/epoch - 2ms/step ## Epoch 189/250 ## 150/150 - 0s - loss: 0.0250 - val_loss: 0.0381 - 310ms/epoch - 2ms/step ## Epoch 190/250 ## 150/150 - 0s - loss: 0.0252 - val_loss: 0.0360 - 319ms/epoch - 2ms/step ## Epoch 191/250 ## 150/150 - 0s - loss: 0.0253 - val_loss: 0.0364 - 306ms/epoch - 2ms/step ## Epoch 192/250 ## 150/150 - 0s - loss: 0.0256 - val_loss: 0.0349 - 312ms/epoch - 2ms/step ## Epoch 193/250 ## 150/150 - 0s - loss: 0.0253 - val_loss: 0.0381 - 309ms/epoch - 2ms/step ## Epoch 194/250 ## 150/150 - 0s - loss: 0.0252 - val_loss: 0.0369 - 370ms/epoch - 2ms/step ## Epoch 195/250 ## 150/150 - 0s - loss: 0.0255 - val_loss: 0.0380 - 316ms/epoch - 2ms/step ## Epoch 196/250 ## 150/150 - 0s - loss: 0.0257 - val_loss: 0.0377 - 322ms/epoch - 2ms/step ## Epoch 197/250 ## 150/150 - 0s - loss: 0.0254 - val_loss: 0.0349 - 306ms/epoch - 2ms/step ## Epoch 198/250 ## 150/150 - 0s - loss: 0.0251 - val_loss: 0.0388 - 285ms/epoch - 2ms/step ## Epoch 199/250 ## 150/150 - 0s - loss: 0.0255 - val_loss: 0.0354 - 303ms/epoch - 2ms/step ## Epoch 200/250 ## 150/150 - 0s - loss: 0.0253 - val_loss: 0.0356 - 295ms/epoch - 2ms/step ## Epoch 201/250 ## 150/150 - 0s - loss: 0.0256 - val_loss: 0.0360 - 327ms/epoch - 2ms/step ## Epoch 202/250 ## 150/150 - 0s - loss: 0.0255 - val_loss: 0.0362 - 307ms/epoch - 2ms/step ## Epoch 203/250 ## 150/150 - 0s - loss: 0.0251 - val_loss: 0.0371 - 299ms/epoch - 2ms/step ## Epoch 204/250 ## 150/150 - 0s - loss: 0.0254 - val_loss: 0.0356 - 294ms/epoch - 2ms/step ## Epoch 205/250 ## 150/150 - 0s - loss: 0.0258 - val_loss: 0.0363 - 293ms/epoch - 2ms/step ## Epoch 206/250 ## 150/150 - 0s - loss: 0.0254 - val_loss: 0.0355 - 294ms/epoch - 2ms/step ## Epoch 207/250 ## 150/150 - 0s - loss: 0.0256 - val_loss: 0.0348 - 296ms/epoch - 2ms/step ## Epoch 208/250 ## 150/150 - 0s - loss: 0.0253 - val_loss: 0.0360 - 290ms/epoch - 2ms/step ## Epoch 209/250 ## 150/150 - 0s - loss: 0.0255 - val_loss: 0.0368 - 295ms/epoch - 2ms/step ## Epoch 210/250 ## 150/150 - 0s - loss: 0.0255 - val_loss: 0.0354 - 312ms/epoch - 2ms/step ## Epoch 211/250 ## 150/150 - 0s - loss: 0.0257 - val_loss: 0.0359 - 304ms/epoch - 2ms/step ## Epoch 212/250 ## 150/150 - 0s - loss: 0.0248 - val_loss: 0.0359 - 295ms/epoch - 2ms/step ## Epoch 213/250 ## 150/150 - 0s - loss: 0.0257 - val_loss: 0.0337 - 295ms/epoch - 2ms/step ## Epoch 214/250 ## 150/150 - 0s - loss: 0.0253 - val_loss: 0.0340 - 293ms/epoch - 2ms/step ## Epoch 215/250 ## 150/150 - 0s - loss: 0.0253 - val_loss: 0.0338 - 299ms/epoch - 2ms/step ## Epoch 216/250 ## 150/150 - 0s - loss: 0.0255 - val_loss: 0.0351 - 289ms/epoch - 2ms/step ## Epoch 217/250 ## 150/150 - 0s - loss: 0.0253 - val_loss: 0.0348 - 302ms/epoch - 2ms/step ## Epoch 218/250 ## 150/150 - 0s - loss: 0.0253 - val_loss: 0.0346 - 297ms/epoch - 2ms/step ## Epoch 219/250 ## 150/150 - 0s - loss: 0.0254 - val_loss: 0.0357 - 389ms/epoch - 3ms/step ## Epoch 220/250 ## 150/150 - 0s - loss: 0.0252 - val_loss: 0.0338 - 295ms/epoch - 2ms/step ## Epoch 221/250 ## 150/150 - 0s - loss: 0.0256 - val_loss: 0.0360 - 296ms/epoch - 2ms/step ## Epoch 222/250 ## 150/150 - 0s - loss: 0.0255 - val_loss: 0.0353 - 465ms/epoch - 3ms/step ## Epoch 223/250 ## 150/150 - 0s - loss: 0.0254 - val_loss: 0.0372 - 313ms/epoch - 2ms/step ## Epoch 224/250 ## 150/150 - 0s - loss: 0.0254 - val_loss: 0.0357 - 291ms/epoch - 2ms/step ## Epoch 225/250 ## 150/150 - 0s - loss: 0.0251 - val_loss: 0.0349 - 290ms/epoch - 2ms/step ## Epoch 226/250 ## 150/150 - 0s - loss: 0.0255 - val_loss: 0.0333 - 304ms/epoch - 2ms/step ## Epoch 227/250 ## 150/150 - 0s - loss: 0.0249 - val_loss: 0.0349 - 294ms/epoch - 2ms/step ## Epoch 228/250 ## 150/150 - 0s - loss: 0.0252 - val_loss: 0.0339 - 330ms/epoch - 2ms/step ## Epoch 229/250 ## 150/150 - 0s - loss: 0.0253 - val_loss: 0.0345 - 303ms/epoch - 2ms/step ## Epoch 230/250 ## 150/150 - 0s - loss: 0.0253 - val_loss: 0.0363 - 288ms/epoch - 2ms/step ## Epoch 231/250 ## 150/150 - 0s - loss: 0.0256 - val_loss: 0.0354 - 288ms/epoch - 2ms/step ## Epoch 232/250 ## 150/150 - 0s - loss: 0.0256 - val_loss: 0.0351 - 286ms/epoch - 2ms/step ## Epoch 233/250 ## 150/150 - 0s - loss: 0.0256 - val_loss: 0.0355 - 289ms/epoch - 2ms/step ## Epoch 234/250 ## 150/150 - 0s - loss: 0.0252 - val_loss: 0.0340 - 286ms/epoch - 2ms/step ## Epoch 235/250 ## 150/150 - 0s - loss: 0.0253 - val_loss: 0.0367 - 287ms/epoch - 2ms/step ## Epoch 236/250 ## 150/150 - 0s - loss: 0.0252 - val_loss: 0.0360 - 304ms/epoch - 2ms/step ## Epoch 237/250 ## 150/150 - 0s - loss: 0.0252 - val_loss: 0.0364 - 290ms/epoch - 2ms/step ## Epoch 238/250 ## 150/150 - 0s - loss: 0.0252 - val_loss: 0.0356 - 285ms/epoch - 2ms/step ## Epoch 239/250 ## 150/150 - 0s - loss: 0.0256 - val_loss: 0.0375 - 290ms/epoch - 2ms/step ## Epoch 240/250 ## 150/150 - 0s - loss: 0.0253 - val_loss: 0.0380 - 291ms/epoch - 2ms/step ## Epoch 241/250 ## 150/150 - 0s - loss: 0.0252 - val_loss: 0.0379 - 288ms/epoch - 2ms/step ## Epoch 242/250 ## 150/150 - 0s - loss: 0.0252 - val_loss: 0.0346 - 308ms/epoch - 2ms/step ## Epoch 243/250 ## 150/150 - 0s - loss: 0.0254 - val_loss: 0.0368 - 390ms/epoch - 3ms/step ## Epoch 244/250 ## 150/150 - 0s - loss: 0.0250 - val_loss: 0.0370 - 300ms/epoch - 2ms/step ## Epoch 245/250 ## 150/150 - 0s - loss: 0.0249 - val_loss: 0.0367 - 286ms/epoch - 2ms/step ## Epoch 246/250 ## 150/150 - 0s - loss: 0.0252 - val_loss: 0.0398 - 308ms/epoch - 2ms/step ## Epoch 247/250 ## 150/150 - 0s - loss: 0.0257 - val_loss: 0.0352 - 310ms/epoch - 2ms/step ## Epoch 248/250 ## 150/150 - 0s - loss: 0.0245 - val_loss: 0.0380 - 313ms/epoch - 2ms/step ## Epoch 249/250 ## 150/150 - 0s - loss: 0.0252 - val_loss: 0.0361 - 340ms/epoch - 2ms/step ## Epoch 250/250 ## 150/150 - 0s - loss: 0.0250 - val_loss: 0.0352 - 392ms/epoch - 3ms/step Después de entrenar la red neuronal estos fueron los resultados. ## 1/1 - 0s - 165ms/epoch - 165ms/step ## RMSE Keras ELMAN: 293.8443 ## MAE Keras ELMAN: 247.5053 En la grafica de real vs predicciones se ve que el modelo se mantiene en el promedio de las ventas reales, con esto se puede concluir que el modelo no es capaz de predecir los comportamientos atipicos de ventas, esto es un resultado parecido al modelo prophet. Figure 6.2: Estrutura red ELMAN Esta gráfica representa la evolución de la función de pérdida a lo largo de las 200 épocas del modelo. El eje x muestra el número de épocas y el eje y muestra el valor de la pérdida. Loss (Línea azul) representa el valor de la función de pérdida (mse) el conjunto de entrenamiento después de cada época. Mide cuán lejos están las predicciones del modelo de los valores reales en los datos con los que se entrena. Val_loss (Línea verde) representa la misma funcion de perdida de loss pero contra el conjunto de validación. Esta linea indica si el modelo está aprendiendo patrones útiles o sobreajustando. Si val_loss aumenta mientras loss disminuye, es una señal de sobreajuste. Ambas líneas disminuyen la perdida rápidamente antes de las 50 epocas y luego se estabilizan. Esto indica que el modelo encuentra una configuración de pesos que minimiza la pérdida. Esto indica que el modelo generaliza bien, ya que la funcion de pérdida en validación no diverge significativamente del valor en entrenamiento. Después de 100 épocas ambas líneas se mantienen relativamente estables con una tendencia muy poco significativa hacia abajo, con algunas fluctuaciones.Esto implica que entrenar más allá de 200 épocas probablemente no mejorará mucho el rendimiento, ya que el modelo ha alcanzado un punto de mínima pérdida. 6.2 Jordan Estructura: Es similar a la ELMAN, pero la capa de contexto almacena las salidas del paso anterior y(t-1) en lugar de las activaciones ocultas como retroalimentación junto a las entradas actuales x(t) para generar las activaciones ocultas h(t) y dar el pronostico del valor actual y(t). Figure 6.3: Estrutura red ELMAN El modelo se entrenó nuevamente usando la librería keras con los siguientes parámetros: units = 30. epochs = 300. batch_size = 40. validation_split = 0.2. ## Epoch 1/300 ## ## 1/6 [====&gt;.........................] - ETA: 4s - loss: 0.1674 ## 6/6 [==============================] - 1s 41ms/step - loss: 0.1033 - val_loss: 0.0633 ## Epoch 2/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0455 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0479 - val_loss: 0.0599 ## Epoch 3/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0434 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0484 - val_loss: 0.0522 ## Epoch 4/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0272 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0363 - val_loss: 0.0613 ## Epoch 5/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0356 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0375 - val_loss: 0.0615 ## Epoch 6/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0359 ## 6/6 [==============================] - 0s 8ms/step - loss: 0.0349 - val_loss: 0.0523 ## Epoch 7/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0373 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0324 - val_loss: 0.0486 ## Epoch 8/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0348 ## 6/6 [==============================] - 0s 8ms/step - loss: 0.0323 - val_loss: 0.0479 ## Epoch 9/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0343 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0311 - val_loss: 0.0495 ## Epoch 10/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0291 ## 6/6 [==============================] - 0s 8ms/step - loss: 0.0303 - val_loss: 0.0482 ## Epoch 11/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0435 ## 6/6 [==============================] - 0s 8ms/step - loss: 0.0297 - val_loss: 0.0465 ## Epoch 12/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0262 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0291 - val_loss: 0.0461 ## Epoch 13/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0299 ## 6/6 [==============================] - 0s 8ms/step - loss: 0.0286 - val_loss: 0.0456 ## Epoch 14/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0338 ## 6/6 [==============================] - 0s 8ms/step - loss: 0.0282 - val_loss: 0.0449 ## Epoch 15/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0176 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0278 - val_loss: 0.0442 ## Epoch 16/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0333 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0275 - val_loss: 0.0434 ## Epoch 17/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0264 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0272 - val_loss: 0.0439 ## Epoch 18/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0194 ## 6/6 [==============================] - 0s 8ms/step - loss: 0.0270 - val_loss: 0.0438 ## Epoch 19/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0181 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0267 - val_loss: 0.0426 ## Epoch 20/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0278 ## 6/6 [==============================] - 0s 8ms/step - loss: 0.0265 - val_loss: 0.0426 ## Epoch 21/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0332 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0266 - val_loss: 0.0425 ## Epoch 22/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0260 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0262 - val_loss: 0.0438 ## Epoch 23/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0415 ## 6/6 [==============================] - 0s 8ms/step - loss: 0.0262 - val_loss: 0.0427 ## Epoch 24/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0269 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0261 - val_loss: 0.0420 ## Epoch 25/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0216 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0259 - val_loss: 0.0424 ## Epoch 26/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0255 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0257 - val_loss: 0.0426 ## Epoch 27/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0333 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0257 - val_loss: 0.0423 ## Epoch 28/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0293 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0256 - val_loss: 0.0415 ## Epoch 29/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0405 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0255 - val_loss: 0.0421 ## Epoch 30/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0269 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0255 - val_loss: 0.0420 ## Epoch 31/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0308 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0255 - val_loss: 0.0422 ## Epoch 32/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0326 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0256 - val_loss: 0.0419 ## Epoch 33/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0273 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0255 - val_loss: 0.0411 ## Epoch 34/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0286 ## 6/6 [==============================] - 0s 8ms/step - loss: 0.0253 - val_loss: 0.0419 ## Epoch 35/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0239 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0255 - val_loss: 0.0424 ## Epoch 36/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0189 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0255 - val_loss: 0.0412 ## Epoch 37/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0301 ## 6/6 [==============================] - 0s 8ms/step - loss: 0.0252 - val_loss: 0.0423 ## Epoch 38/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0238 ## 6/6 [==============================] - 0s 8ms/step - loss: 0.0253 - val_loss: 0.0422 ## Epoch 39/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0315 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0257 - val_loss: 0.0411 ## Epoch 40/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0247 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0251 - val_loss: 0.0423 ## Epoch 41/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0249 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0255 - val_loss: 0.0417 ## Epoch 42/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0250 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0253 - val_loss: 0.0414 ## Epoch 43/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0233 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0251 - val_loss: 0.0425 ## Epoch 44/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0268 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0258 - val_loss: 0.0413 ## Epoch 45/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0253 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0251 - val_loss: 0.0412 ## Epoch 46/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0197 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0252 - val_loss: 0.0418 ## Epoch 47/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0281 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0252 - val_loss: 0.0422 ## Epoch 48/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0252 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0251 - val_loss: 0.0415 ## Epoch 49/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0192 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0253 - val_loss: 0.0423 ## Epoch 50/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0308 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0251 - val_loss: 0.0415 ## Epoch 51/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0225 ## 6/6 [==============================] - 0s 8ms/step - loss: 0.0251 - val_loss: 0.0417 ## Epoch 52/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0316 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0251 - val_loss: 0.0411 ## Epoch 53/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0312 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0251 - val_loss: 0.0420 ## Epoch 54/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0311 ## 6/6 [==============================] - 0s 8ms/step - loss: 0.0251 - val_loss: 0.0415 ## Epoch 55/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0228 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0251 - val_loss: 0.0411 ## Epoch 56/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0285 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0251 - val_loss: 0.0421 ## Epoch 57/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0230 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0253 - val_loss: 0.0417 ## Epoch 58/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0228 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0251 - val_loss: 0.0414 ## Epoch 59/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0298 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0252 - val_loss: 0.0416 ## Epoch 60/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0232 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0251 - val_loss: 0.0415 ## Epoch 61/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0336 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0251 - val_loss: 0.0419 ## Epoch 62/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0263 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0251 - val_loss: 0.0409 ## Epoch 63/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0248 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0251 - val_loss: 0.0413 ## Epoch 64/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0228 ## 6/6 [==============================] - 0s 8ms/step - loss: 0.0251 - val_loss: 0.0415 ## Epoch 65/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0277 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0251 - val_loss: 0.0424 ## Epoch 66/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0287 ## 6/6 [==============================] - 0s 8ms/step - loss: 0.0250 - val_loss: 0.0413 ## Epoch 67/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0352 ## 6/6 [==============================] - 0s 8ms/step - loss: 0.0251 - val_loss: 0.0412 ## Epoch 68/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0251 ## 6/6 [==============================] - 0s 8ms/step - loss: 0.0251 - val_loss: 0.0420 ## Epoch 69/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0284 ## 6/6 [==============================] - 0s 9ms/step - loss: 0.0252 - val_loss: 0.0420 ## Epoch 70/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0242 ## 6/6 [==============================] - 0s 9ms/step - loss: 0.0250 - val_loss: 0.0409 ## Epoch 71/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0245 ## 6/6 [==============================] - 0s 8ms/step - loss: 0.0252 - val_loss: 0.0412 ## Epoch 72/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0233 ## 6/6 [==============================] - 0s 8ms/step - loss: 0.0262 - val_loss: 0.0433 ## Epoch 73/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0268 ## 6/6 [==============================] - 0s 8ms/step - loss: 0.0253 - val_loss: 0.0408 ## Epoch 74/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0286 ## 6/6 [==============================] - 0s 8ms/step - loss: 0.0252 - val_loss: 0.0417 ## Epoch 75/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0238 ## 6/6 [==============================] - 0s 8ms/step - loss: 0.0251 - val_loss: 0.0423 ## Epoch 76/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0238 ## 6/6 [==============================] - 0s 8ms/step - loss: 0.0251 - val_loss: 0.0414 ## Epoch 77/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0289 ## 6/6 [==============================] - 0s 8ms/step - loss: 0.0253 - val_loss: 0.0417 ## Epoch 78/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0272 ## 6/6 [==============================] - 0s 8ms/step - loss: 0.0256 - val_loss: 0.0434 ## Epoch 79/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0219 ## 6/6 [==============================] - 0s 8ms/step - loss: 0.0250 - val_loss: 0.0406 ## Epoch 80/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0215 ## 6/6 [==============================] - 0s 8ms/step - loss: 0.0252 - val_loss: 0.0409 ## Epoch 81/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0249 ## 6/6 [==============================] - 0s 8ms/step - loss: 0.0251 - val_loss: 0.0418 ## Epoch 82/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0227 ## 6/6 [==============================] - 0s 8ms/step - loss: 0.0252 - val_loss: 0.0410 ## Epoch 83/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0173 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0251 - val_loss: 0.0422 ## Epoch 84/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0225 ## 6/6 [==============================] - 0s 8ms/step - loss: 0.0253 - val_loss: 0.0411 ## Epoch 85/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0221 ## 6/6 [==============================] - 0s 9ms/step - loss: 0.0250 - val_loss: 0.0420 ## Epoch 86/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0275 ## 6/6 [==============================] - 0s 8ms/step - loss: 0.0250 - val_loss: 0.0417 ## Epoch 87/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0231 ## 6/6 [==============================] - 0s 9ms/step - loss: 0.0260 - val_loss: 0.0407 ## Epoch 88/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0187 ## 6/6 [==============================] - 0s 8ms/step - loss: 0.0253 - val_loss: 0.0434 ## Epoch 89/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0276 ## 6/6 [==============================] - 0s 8ms/step - loss: 0.0252 - val_loss: 0.0409 ## Epoch 90/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0222 ## 6/6 [==============================] - 0s 8ms/step - loss: 0.0250 - val_loss: 0.0404 ## Epoch 91/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0293 ## 6/6 [==============================] - 0s 8ms/step - loss: 0.0252 - val_loss: 0.0420 ## Epoch 92/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0248 ## 6/6 [==============================] - 0s 9ms/step - loss: 0.0250 - val_loss: 0.0412 ## Epoch 93/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0238 ## 6/6 [==============================] - 0s 8ms/step - loss: 0.0252 - val_loss: 0.0413 ## Epoch 94/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0233 ## 6/6 [==============================] - 0s 13ms/step - loss: 0.0253 - val_loss: 0.0425 ## Epoch 95/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0174 ## 6/6 [==============================] - 0s 11ms/step - loss: 0.0250 - val_loss: 0.0409 ## Epoch 96/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0237 ## 6/6 [==============================] - 0s 8ms/step - loss: 0.0251 - val_loss: 0.0411 ## Epoch 97/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0293 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0251 - val_loss: 0.0416 ## Epoch 98/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0323 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0252 - val_loss: 0.0406 ## Epoch 99/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0224 ## 6/6 [==============================] - 0s 8ms/step - loss: 0.0253 - val_loss: 0.0416 ## Epoch 100/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0215 ## 6/6 [==============================] - 0s 8ms/step - loss: 0.0249 - val_loss: 0.0413 ## Epoch 101/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0224 ## 6/6 [==============================] - 0s 8ms/step - loss: 0.0251 - val_loss: 0.0414 ## Epoch 102/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0229 ## 6/6 [==============================] - 0s 8ms/step - loss: 0.0251 - val_loss: 0.0421 ## Epoch 103/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0238 ## 6/6 [==============================] - 0s 8ms/step - loss: 0.0249 - val_loss: 0.0411 ## Epoch 104/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0215 ## 6/6 [==============================] - 0s 8ms/step - loss: 0.0249 - val_loss: 0.0411 ## Epoch 105/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0238 ## 6/6 [==============================] - 0s 9ms/step - loss: 0.0251 - val_loss: 0.0418 ## Epoch 106/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0263 ## 6/6 [==============================] - 0s 8ms/step - loss: 0.0249 - val_loss: 0.0407 ## Epoch 107/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0281 ## 6/6 [==============================] - 0s 8ms/step - loss: 0.0250 - val_loss: 0.0411 ## Epoch 108/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0254 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0250 - val_loss: 0.0418 ## Epoch 109/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0266 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0250 - val_loss: 0.0416 ## Epoch 110/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0278 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0250 - val_loss: 0.0411 ## Epoch 111/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0262 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0252 - val_loss: 0.0407 ## Epoch 112/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0268 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0250 - val_loss: 0.0421 ## Epoch 113/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0339 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0251 - val_loss: 0.0412 ## Epoch 114/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0299 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0251 - val_loss: 0.0414 ## Epoch 115/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0278 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0250 - val_loss: 0.0411 ## Epoch 116/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0265 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0249 - val_loss: 0.0415 ## Epoch 117/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0302 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0251 - val_loss: 0.0417 ## Epoch 118/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0265 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0250 - val_loss: 0.0407 ## Epoch 119/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0267 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0251 - val_loss: 0.0414 ## Epoch 120/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0232 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0250 - val_loss: 0.0406 ## Epoch 121/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0235 ## 6/6 [==============================] - 0s 8ms/step - loss: 0.0250 - val_loss: 0.0411 ## Epoch 122/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0306 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0249 - val_loss: 0.0412 ## Epoch 123/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0237 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0250 - val_loss: 0.0412 ## Epoch 124/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0209 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0248 - val_loss: 0.0417 ## Epoch 125/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0256 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0250 - val_loss: 0.0415 ## Epoch 126/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0190 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0250 - val_loss: 0.0413 ## Epoch 127/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0231 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0250 - val_loss: 0.0413 ## Epoch 128/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0231 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0249 - val_loss: 0.0415 ## Epoch 129/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0312 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0249 - val_loss: 0.0412 ## Epoch 130/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0284 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0249 - val_loss: 0.0412 ## Epoch 131/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0195 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0250 - val_loss: 0.0417 ## Epoch 132/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0273 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0249 - val_loss: 0.0409 ## Epoch 133/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0229 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0252 - val_loss: 0.0408 ## Epoch 134/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0296 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0249 - val_loss: 0.0416 ## Epoch 135/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0331 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0253 - val_loss: 0.0412 ## Epoch 136/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0180 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0251 - val_loss: 0.0423 ## Epoch 137/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0187 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0252 - val_loss: 0.0408 ## Epoch 138/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0223 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0249 - val_loss: 0.0414 ## Epoch 139/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0241 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0249 - val_loss: 0.0419 ## Epoch 140/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0195 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0250 - val_loss: 0.0408 ## Epoch 141/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0256 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0250 - val_loss: 0.0408 ## Epoch 142/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0240 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0252 - val_loss: 0.0418 ## Epoch 143/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0288 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0249 - val_loss: 0.0407 ## Epoch 144/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0270 ## 6/6 [==============================] - 0s 8ms/step - loss: 0.0250 - val_loss: 0.0417 ## Epoch 145/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0235 ## 6/6 [==============================] - 0s 8ms/step - loss: 0.0263 - val_loss: 0.0429 ## Epoch 146/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0241 ## 6/6 [==============================] - 0s 8ms/step - loss: 0.0247 - val_loss: 0.0404 ## Epoch 147/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0188 ## 6/6 [==============================] - 0s 8ms/step - loss: 0.0255 - val_loss: 0.0416 ## Epoch 148/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0224 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0252 - val_loss: 0.0415 ## Epoch 149/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0256 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0248 - val_loss: 0.0409 ## Epoch 150/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0266 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0250 - val_loss: 0.0409 ## Epoch 151/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0270 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0253 - val_loss: 0.0422 ## Epoch 152/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0198 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0250 - val_loss: 0.0405 ## Epoch 153/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0322 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0252 - val_loss: 0.0413 ## Epoch 154/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0257 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0249 - val_loss: 0.0413 ## Epoch 155/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0370 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0250 - val_loss: 0.0404 ## Epoch 156/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0258 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0251 - val_loss: 0.0417 ## Epoch 157/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0228 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0249 - val_loss: 0.0407 ## Epoch 158/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0213 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0250 - val_loss: 0.0416 ## Epoch 159/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0236 ## 6/6 [==============================] - 0s 8ms/step - loss: 0.0249 - val_loss: 0.0410 ## Epoch 160/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0192 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0250 - val_loss: 0.0418 ## Epoch 161/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0276 ## 6/6 [==============================] - 0s 8ms/step - loss: 0.0248 - val_loss: 0.0407 ## Epoch 162/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0226 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0250 - val_loss: 0.0410 ## Epoch 163/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0183 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0249 - val_loss: 0.0402 ## Epoch 164/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0211 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0249 - val_loss: 0.0419 ## Epoch 165/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0225 ## 6/6 [==============================] - 0s 8ms/step - loss: 0.0250 - val_loss: 0.0409 ## Epoch 166/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0184 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0250 - val_loss: 0.0419 ## Epoch 167/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0261 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0249 - val_loss: 0.0407 ## Epoch 168/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0220 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0251 - val_loss: 0.0408 ## Epoch 169/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0209 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0247 - val_loss: 0.0419 ## Epoch 170/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0251 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0249 - val_loss: 0.0413 ## Epoch 171/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0281 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0248 - val_loss: 0.0409 ## Epoch 172/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0220 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0249 - val_loss: 0.0414 ## Epoch 173/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0210 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0249 - val_loss: 0.0409 ## Epoch 174/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0211 ## 6/6 [==============================] - 0s 8ms/step - loss: 0.0248 - val_loss: 0.0415 ## Epoch 175/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0235 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0247 - val_loss: 0.0413 ## Epoch 176/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0230 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0249 - val_loss: 0.0407 ## Epoch 177/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0200 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0250 - val_loss: 0.0414 ## Epoch 178/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0226 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0251 - val_loss: 0.0402 ## Epoch 179/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0330 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0248 - val_loss: 0.0416 ## Epoch 180/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0330 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0248 - val_loss: 0.0413 ## Epoch 181/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0218 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0248 - val_loss: 0.0411 ## Epoch 182/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0268 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0248 - val_loss: 0.0415 ## Epoch 183/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0253 ## 6/6 [==============================] - 0s 9ms/step - loss: 0.0248 - val_loss: 0.0408 ## Epoch 184/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0259 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0248 - val_loss: 0.0410 ## Epoch 185/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0216 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0250 - val_loss: 0.0418 ## Epoch 186/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0360 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0251 - val_loss: 0.0404 ## Epoch 187/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0252 ## 6/6 [==============================] - 0s 8ms/step - loss: 0.0252 - val_loss: 0.0426 ## Epoch 188/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0260 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0247 - val_loss: 0.0409 ## Epoch 189/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0322 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0249 - val_loss: 0.0410 ## Epoch 190/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0288 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0248 - val_loss: 0.0416 ## Epoch 191/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0260 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0249 - val_loss: 0.0407 ## Epoch 192/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0199 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0249 - val_loss: 0.0414 ## Epoch 193/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0192 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0247 - val_loss: 0.0413 ## Epoch 194/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0227 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0251 - val_loss: 0.0411 ## Epoch 195/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0312 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0248 - val_loss: 0.0416 ## Epoch 196/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0286 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0248 - val_loss: 0.0408 ## Epoch 197/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0238 ## 6/6 [==============================] - 0s 8ms/step - loss: 0.0248 - val_loss: 0.0412 ## Epoch 198/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0235 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0249 - val_loss: 0.0411 ## Epoch 199/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0229 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0252 - val_loss: 0.0419 ## Epoch 200/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0304 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0252 - val_loss: 0.0407 ## Epoch 201/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0271 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0250 - val_loss: 0.0421 ## Epoch 202/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0190 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0247 - val_loss: 0.0411 ## Epoch 203/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0260 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0248 - val_loss: 0.0406 ## Epoch 204/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0283 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0247 - val_loss: 0.0408 ## Epoch 205/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0271 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0247 - val_loss: 0.0416 ## Epoch 206/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0223 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0248 - val_loss: 0.0408 ## Epoch 207/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0248 ## 6/6 [==============================] - 0s 8ms/step - loss: 0.0251 - val_loss: 0.0409 ## Epoch 208/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0209 ## 6/6 [==============================] - 0s 8ms/step - loss: 0.0248 - val_loss: 0.0421 ## Epoch 209/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0331 ## 6/6 [==============================] - 0s 8ms/step - loss: 0.0249 - val_loss: 0.0409 ## Epoch 210/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0238 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0248 - val_loss: 0.0415 ## Epoch 211/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0216 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0248 - val_loss: 0.0414 ## Epoch 212/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0194 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0247 - val_loss: 0.0409 ## Epoch 213/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0225 ## 6/6 [==============================] - 0s 8ms/step - loss: 0.0247 - val_loss: 0.0409 ## Epoch 214/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0237 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0247 - val_loss: 0.0415 ## Epoch 215/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0238 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0250 - val_loss: 0.0415 ## Epoch 216/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0240 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0245 - val_loss: 0.0407 ## Epoch 217/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0290 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0248 - val_loss: 0.0412 ## Epoch 218/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0324 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0247 - val_loss: 0.0411 ## Epoch 219/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0290 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0258 - val_loss: 0.0425 ## Epoch 220/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0235 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0250 - val_loss: 0.0405 ## Epoch 221/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0262 ## 6/6 [==============================] - 0s 9ms/step - loss: 0.0249 - val_loss: 0.0423 ## Epoch 222/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0253 ## 6/6 [==============================] - 0s 8ms/step - loss: 0.0248 - val_loss: 0.0410 ## Epoch 223/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0259 ## 6/6 [==============================] - 0s 8ms/step - loss: 0.0246 - val_loss: 0.0399 ## Epoch 224/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0321 ## 6/6 [==============================] - 0s 8ms/step - loss: 0.0249 - val_loss: 0.0414 ## Epoch 225/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0221 ## 6/6 [==============================] - 0s 8ms/step - loss: 0.0249 - val_loss: 0.0417 ## Epoch 226/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0273 ## 6/6 [==============================] - 0s 8ms/step - loss: 0.0247 - val_loss: 0.0410 ## Epoch 227/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0210 ## 6/6 [==============================] - 0s 9ms/step - loss: 0.0246 - val_loss: 0.0419 ## Epoch 228/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0282 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0247 - val_loss: 0.0413 ## Epoch 229/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0239 ## 6/6 [==============================] - 0s 8ms/step - loss: 0.0248 - val_loss: 0.0414 ## Epoch 230/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0225 ## 6/6 [==============================] - 0s 8ms/step - loss: 0.0247 - val_loss: 0.0410 ## Epoch 231/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0184 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0247 - val_loss: 0.0406 ## Epoch 232/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0219 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0248 - val_loss: 0.0418 ## Epoch 233/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0230 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0249 - val_loss: 0.0406 ## Epoch 234/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0233 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0246 - val_loss: 0.0414 ## Epoch 235/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0319 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0249 - val_loss: 0.0417 ## Epoch 236/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0216 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0244 - val_loss: 0.0405 ## Epoch 237/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0364 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0247 - val_loss: 0.0413 ## Epoch 238/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0320 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0249 - val_loss: 0.0422 ## Epoch 239/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0206 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0245 - val_loss: 0.0406 ## Epoch 240/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0274 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0252 - val_loss: 0.0418 ## Epoch 241/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0199 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0245 - val_loss: 0.0397 ## Epoch 242/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0242 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0249 - val_loss: 0.0417 ## Epoch 243/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0230 ## 6/6 [==============================] - 0s 8ms/step - loss: 0.0246 - val_loss: 0.0422 ## Epoch 244/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0259 ## 6/6 [==============================] - 0s 14ms/step - loss: 0.0245 - val_loss: 0.0412 ## Epoch 245/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0190 ## 6/6 [==============================] - 0s 10ms/step - loss: 0.0248 - val_loss: 0.0409 ## Epoch 246/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0250 ## 6/6 [==============================] - 0s 8ms/step - loss: 0.0250 - val_loss: 0.0418 ## Epoch 247/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0228 ## 6/6 [==============================] - 0s 8ms/step - loss: 0.0252 - val_loss: 0.0400 ## Epoch 248/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0181 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0247 - val_loss: 0.0430 ## Epoch 249/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0201 ## 6/6 [==============================] - 0s 8ms/step - loss: 0.0246 - val_loss: 0.0413 ## Epoch 250/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0357 ## 6/6 [==============================] - 0s 8ms/step - loss: 0.0252 - val_loss: 0.0411 ## Epoch 251/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0275 ## 6/6 [==============================] - 0s 8ms/step - loss: 0.0245 - val_loss: 0.0416 ## Epoch 252/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0294 ## 6/6 [==============================] - 0s 8ms/step - loss: 0.0247 - val_loss: 0.0404 ## Epoch 253/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0246 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0245 - val_loss: 0.0416 ## Epoch 254/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0231 ## 6/6 [==============================] - 0s 9ms/step - loss: 0.0246 - val_loss: 0.0415 ## Epoch 255/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0264 ## 6/6 [==============================] - 0s 9ms/step - loss: 0.0246 - val_loss: 0.0404 ## Epoch 256/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0306 ## 6/6 [==============================] - 0s 9ms/step - loss: 0.0245 - val_loss: 0.0423 ## Epoch 257/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0244 ## 6/6 [==============================] - 0s 9ms/step - loss: 0.0250 - val_loss: 0.0406 ## Epoch 258/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0237 ## 6/6 [==============================] - 0s 9ms/step - loss: 0.0245 - val_loss: 0.0416 ## Epoch 259/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0264 ## 6/6 [==============================] - 0s 9ms/step - loss: 0.0247 - val_loss: 0.0412 ## Epoch 260/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0197 ## 6/6 [==============================] - 0s 9ms/step - loss: 0.0244 - val_loss: 0.0421 ## Epoch 261/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0223 ## 6/6 [==============================] - 0s 9ms/step - loss: 0.0246 - val_loss: 0.0408 ## Epoch 262/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0168 ## 6/6 [==============================] - 0s 8ms/step - loss: 0.0246 - val_loss: 0.0411 ## Epoch 263/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0244 ## 6/6 [==============================] - 0s 8ms/step - loss: 0.0245 - val_loss: 0.0422 ## Epoch 264/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0217 ## 6/6 [==============================] - 0s 8ms/step - loss: 0.0246 - val_loss: 0.0400 ## Epoch 265/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0262 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0247 - val_loss: 0.0404 ## Epoch 266/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0218 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0244 - val_loss: 0.0415 ## Epoch 267/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0227 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0246 - val_loss: 0.0417 ## Epoch 268/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0207 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0244 - val_loss: 0.0410 ## Epoch 269/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0254 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0247 - val_loss: 0.0409 ## Epoch 270/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0228 ## 6/6 [==============================] - 0s 8ms/step - loss: 0.0250 - val_loss: 0.0425 ## Epoch 271/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0251 ## 6/6 [==============================] - 0s 8ms/step - loss: 0.0249 - val_loss: 0.0408 ## Epoch 272/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0216 ## 6/6 [==============================] - 0s 8ms/step - loss: 0.0245 - val_loss: 0.0420 ## Epoch 273/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0249 ## 6/6 [==============================] - 0s 8ms/step - loss: 0.0247 - val_loss: 0.0410 ## Epoch 274/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0246 ## 6/6 [==============================] - 0s 8ms/step - loss: 0.0244 - val_loss: 0.0407 ## Epoch 275/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0270 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0245 - val_loss: 0.0413 ## Epoch 276/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0262 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0245 - val_loss: 0.0412 ## Epoch 277/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0246 ## 6/6 [==============================] - 0s 8ms/step - loss: 0.0245 - val_loss: 0.0417 ## Epoch 278/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0241 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0250 - val_loss: 0.0403 ## Epoch 279/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0184 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0243 - val_loss: 0.0430 ## Epoch 280/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0173 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0244 - val_loss: 0.0403 ## Epoch 281/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0223 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0252 - val_loss: 0.0407 ## Epoch 282/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0194 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0249 - val_loss: 0.0416 ## Epoch 283/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0250 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0249 - val_loss: 0.0409 ## Epoch 284/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0186 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0246 - val_loss: 0.0424 ## Epoch 285/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0231 ## 6/6 [==============================] - 0s 8ms/step - loss: 0.0243 - val_loss: 0.0408 ## Epoch 286/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0261 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0247 - val_loss: 0.0411 ## Epoch 287/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0234 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0244 - val_loss: 0.0402 ## Epoch 288/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0294 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0244 - val_loss: 0.0415 ## Epoch 289/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0195 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0244 - val_loss: 0.0406 ## Epoch 290/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0243 ## 6/6 [==============================] - 0s 8ms/step - loss: 0.0246 - val_loss: 0.0408 ## Epoch 291/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0268 ## 6/6 [==============================] - 0s 8ms/step - loss: 0.0245 - val_loss: 0.0413 ## Epoch 292/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0314 ## 6/6 [==============================] - 0s 8ms/step - loss: 0.0243 - val_loss: 0.0402 ## Epoch 293/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0270 ## 6/6 [==============================] - 0s 8ms/step - loss: 0.0244 - val_loss: 0.0403 ## Epoch 294/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0226 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0251 - val_loss: 0.0423 ## Epoch 295/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0258 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0255 - val_loss: 0.0404 ## Epoch 296/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0244 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0247 - val_loss: 0.0428 ## Epoch 297/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0292 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0252 - val_loss: 0.0397 ## Epoch 298/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0216 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0253 - val_loss: 0.0419 ## Epoch 299/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0214 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0247 - val_loss: 0.0395 ## Epoch 300/300 ## ## 1/6 [====&gt;.........................] - ETA: 0s - loss: 0.0251 ## 6/6 [==============================] - 0s 7ms/step - loss: 0.0242 - val_loss: 0.0423 ## 1/1 - 0s - 127ms/epoch - 127ms/step Después de entrenar la red neuronal estos fueron los resultados. ## RMSE Keras Jordan: 324.6018 ## MAE Keras Jordan: 261.7301 Igual que en la grafica de ELMAN el modelo se mantiene en el promedio de las ventas reales y tiene el mismo problema para predecir los comportamientos atipicos de ventas. Figure 6.4: Estrutura red ELMAN Esta gráfica representa la evolución de la función de pérdida del modelo Jordan, su comportamiento es muy similar al caso de ELMAN con la diferencia que los valores iniciales de perdida fueron más bajos (0.05) que los de ELMAN (0.08). Se aumentó la cantidad de epocas (300) buscando una mejor función de perdida, aunque el resutlado de este modelo es mejor al de ELMAN el resultado sigue siendo menos favorable que usar SARIMA. Igual que en ELMAN ambas líneas disminuyen la perdida rápidamente antes de las 50 epocas y luego se estabilizan. Después de 100 épocas ambas líneas se mantienen relativamente estables con una tendencia muy poco significativa hacia abajo, con algunas fluctuaciones. 6.3 Resultados En esta tabla se comparan los resultados de los modelos de las redes ELMAN y Jordan contra los demás modelos usado en el curso. ## RMSE MAE ## Holt-Winters Aditivo 140.4213 111.8802 ## Holt-Winters Multiplicativo 143.8373 115.8820 ## SARIMA 137.7336 109.1637 ## Prophet sin regresores 196.8354 146.5227 ## Prophet con regresores 176.8185 134.5342 ## Red ELMAN 335.8276 276.9099 ## Red Jordan 301.6187 246.2491 Como se ve, ambos modelos de las redes neuronales fueron las menos efectivas en su pronostico. Entra las posibles causas están el tamaño limitado del conjunto de datos, el conjunto solo contró con 381 días para entrenar la red cuando normalmente se usan grandes cantidades de datos para aprender patrones complejos y evitar sobreajuste. Esta poca cantidad pudo crear un sobreajuste en el modelo que se nota al ver que val_loss no disminuye en ninguno de los modelos Ademas, para poder hacer el entrenamiento fue necesario agregar los días faltantes e imputar los valores, aunque fueron pocos valores estos pudo volverse ruido. Isaienkov, Yaroslav. 2025. “Coffee Sales.” Kaggle. https://www.kaggle.com/datasets/ihelon/coffee-sales. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
